{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"optiseek An open source collection of single-objective optimization algorithms for multi-dimensional functions. The purpose of this library is to give users access to a variety of optimization algorithms with extreme ease of use and interoperability. The parameters of each of the algorithms can be tuned by the users and there is a high level of input uniformity between algorithms of similar type. Installation $ pip install optiseek Usage optiseek provides access to numerous optimization algorithms that require minimal effort from the user. An example using the well-known particle swarm optimization algorithm can be as simple as this: from optiseek.metaheuristics import particle_swarm_optimizer from optiseek.testfunctions import booth # create an instance of the algorithm, set its parameters, and solve my_algorithm = particle_swarm_optimizer(booth) # create instance to optimize the booth function my_algorithm.b_lower = [-10, -10] # define lower bounds my_algorithm.b_upper = [10, 10] # define upper bounds # execute the algorithm my_algorithm.solve() # show the results! print(my_algorithm.best_value) print(my_algorithm.best_position) print(my_algorithm.completed_iter) This is a fairly basic example implementation without much thought put into parameter selection. Of course, the user is free to tune the parameters of the algorithm any way they would like. License optiseek was created by Alex Dundore. It is licensed under the terms of the MIT license. Credits and Dependencies optiseek is powered by numpy .","title":"Home"},{"location":"#optiseek","text":"An open source collection of single-objective optimization algorithms for multi-dimensional functions. The purpose of this library is to give users access to a variety of optimization algorithms with extreme ease of use and interoperability. The parameters of each of the algorithms can be tuned by the users and there is a high level of input uniformity between algorithms of similar type.","title":"optiseek"},{"location":"#installation","text":"$ pip install optiseek","title":"Installation"},{"location":"#usage","text":"optiseek provides access to numerous optimization algorithms that require minimal effort from the user. An example using the well-known particle swarm optimization algorithm can be as simple as this: from optiseek.metaheuristics import particle_swarm_optimizer from optiseek.testfunctions import booth # create an instance of the algorithm, set its parameters, and solve my_algorithm = particle_swarm_optimizer(booth) # create instance to optimize the booth function my_algorithm.b_lower = [-10, -10] # define lower bounds my_algorithm.b_upper = [10, 10] # define upper bounds # execute the algorithm my_algorithm.solve() # show the results! print(my_algorithm.best_value) print(my_algorithm.best_position) print(my_algorithm.completed_iter) This is a fairly basic example implementation without much thought put into parameter selection. Of course, the user is free to tune the parameters of the algorithm any way they would like.","title":"Usage"},{"location":"#license","text":"optiseek was created by Alex Dundore. It is licensed under the terms of the MIT license.","title":"License"},{"location":"#credits-and-dependencies","text":"optiseek is powered by numpy .","title":"Credits and Dependencies"},{"location":"ackley2D/","text":"Ackley's Function (2-Dimensional) This is a non-convex function with many local optima around a single global minimum of zero at [0, 0]. Form of the function is as follows: f(x, y) = -20exp(-0.2sqrt(0.5(x1^2+x2^2))) - exp(0.5(cos(2\u03c0x1) + cos(2\u03c0x2))) + exp(1) + 20 function optiseek.testfunctions. ackley2D ( x1, x2 ) Parameters Parameter Description x1 : float Input value for the first dimension. x2 : float Input value for the second dimension. Example from optiseek.testfunctions import ackley2D y = ackley2D(0, 0) References List of Test Functions on Wikipedia","title":"Ackleys Function (2D)"},{"location":"ackley2D/#ackleys-function-2-dimensional","text":"This is a non-convex function with many local optima around a single global minimum of zero at [0, 0]. Form of the function is as follows: f(x, y) = -20exp(-0.2sqrt(0.5(x1^2+x2^2))) - exp(0.5(cos(2\u03c0x1) + cos(2\u03c0x2))) + exp(1) + 20 function optiseek.testfunctions. ackley2D ( x1, x2 )","title":"Ackley's Function (2-Dimensional)"},{"location":"ackley2D/#parameters","text":"Parameter Description x1 : float Input value for the first dimension. x2 : float Input value for the second dimension.","title":"Parameters"},{"location":"ackley2D/#example","text":"from optiseek.testfunctions import ackley2D y = ackley2D(0, 0)","title":"Example"},{"location":"ackley2D/#references","text":"List of Test Functions on Wikipedia","title":"References"},{"location":"basic_pattern_search/","text":"Basic Pattern Search This class represents a basic Hooke-Jeeves pattern search algorithm. This is a basic black-box optimization function that requires no knowledge of the form of the function to be optimized. The algorithm starts by selecting two sample points (one on each side of the current position, +/- the step size) for each dimension. If the best sample point is better than the current position, the algorithm sets the current position to the best sample point. Otherwise, the step size halves and the algorithm continues iterating. This method is effective and requires 2n function evaluations for each iteration, where n is the number of dimensions. It is also susceptible to getting stuck in local optima. class optiseek.metaheuristics. basic_pattern_search ( input_function, initial_guess, find_minimum=True, max_iter=100, sol_threshold=None, store_results=False, max_step_size=1.0 ) Parameters Parameter Description input_function : function Function that the algorithm will use to search for an optimum. *args will be passed to the function within the solver. initial_guess : float, list of floats, or ndarray The initial guess that the algorithm will start the search from. Can be a float if the function is one-dimensional. find_minimum : bool Indicates whether the optimimum of interest is a minimum or maximum. If true, looks for minimum. If false, looks for maximum. max_iter : int Maximum number of iterations. If reached, the algorithm terminates. sol_threshold : float If a solution is found better than this threshold, the iterations stop. None indicates that the algorithm will not consider this. store_results : bool Choose whether to save intermediate iteration results for post-processing or not. If true, results will be saved. max_step_size : float Maximum step size that the algorithm can possibly take for each iteration in each direction. Attributes Attribute Description best_position : ndarray Most optimal position found during the solution iterations. best_value : float Most optimal function value found during the solution iterations. completed_iter : int Number of iterations completed during the solution process. stored_positions : ndarray Positions for each member of the population for each iteration after the solver is finished. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, position in each dimension] stored_values : ndarray Function values for each member of the population for each iteration. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, function value] Methods .solve() Executes the algorithm solution with the current parameters. Results will be stored to the class attributes. If the user opted to store intermediate results, these will also be stored. Parameters None Returns None Example from optiseek.direct import basic_pattern_search from optiseek.testfunctions import booth # create an instance of the algorithm, set its parameters, and solve alg = basic_pattern_search(booth, [0, 0]) # create instance with booth test function and initial guess [0, 0] alg.max_iter = 100 # set iteration limit alg.sol_threshold = 0.001 # set a solution threshold alg.max_step_size = 0.5 # define maximum step size # execute the algorithm alg.solve() # show the results! print(alg.best_value) print(alg.best_position) print(alg.completed_iter) References Hooke-Jeeves Pattern Search on Wikipedia","title":"Basic Pattern Search"},{"location":"basic_pattern_search/#basic-pattern-search","text":"This class represents a basic Hooke-Jeeves pattern search algorithm. This is a basic black-box optimization function that requires no knowledge of the form of the function to be optimized. The algorithm starts by selecting two sample points (one on each side of the current position, +/- the step size) for each dimension. If the best sample point is better than the current position, the algorithm sets the current position to the best sample point. Otherwise, the step size halves and the algorithm continues iterating. This method is effective and requires 2n function evaluations for each iteration, where n is the number of dimensions. It is also susceptible to getting stuck in local optima. class optiseek.metaheuristics. basic_pattern_search ( input_function, initial_guess, find_minimum=True, max_iter=100, sol_threshold=None, store_results=False, max_step_size=1.0 )","title":"Basic Pattern Search"},{"location":"basic_pattern_search/#parameters","text":"Parameter Description input_function : function Function that the algorithm will use to search for an optimum. *args will be passed to the function within the solver. initial_guess : float, list of floats, or ndarray The initial guess that the algorithm will start the search from. Can be a float if the function is one-dimensional. find_minimum : bool Indicates whether the optimimum of interest is a minimum or maximum. If true, looks for minimum. If false, looks for maximum. max_iter : int Maximum number of iterations. If reached, the algorithm terminates. sol_threshold : float If a solution is found better than this threshold, the iterations stop. None indicates that the algorithm will not consider this. store_results : bool Choose whether to save intermediate iteration results for post-processing or not. If true, results will be saved. max_step_size : float Maximum step size that the algorithm can possibly take for each iteration in each direction.","title":"Parameters"},{"location":"basic_pattern_search/#attributes","text":"Attribute Description best_position : ndarray Most optimal position found during the solution iterations. best_value : float Most optimal function value found during the solution iterations. completed_iter : int Number of iterations completed during the solution process. stored_positions : ndarray Positions for each member of the population for each iteration after the solver is finished. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, position in each dimension] stored_values : ndarray Function values for each member of the population for each iteration. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, function value]","title":"Attributes"},{"location":"basic_pattern_search/#methods","text":".solve() Executes the algorithm solution with the current parameters. Results will be stored to the class attributes. If the user opted to store intermediate results, these will also be stored. Parameters None Returns None","title":"Methods"},{"location":"basic_pattern_search/#example","text":"from optiseek.direct import basic_pattern_search from optiseek.testfunctions import booth # create an instance of the algorithm, set its parameters, and solve alg = basic_pattern_search(booth, [0, 0]) # create instance with booth test function and initial guess [0, 0] alg.max_iter = 100 # set iteration limit alg.sol_threshold = 0.001 # set a solution threshold alg.max_step_size = 0.5 # define maximum step size # execute the algorithm alg.solve() # show the results! print(alg.best_value) print(alg.best_position) print(alg.completed_iter)","title":"Example"},{"location":"basic_pattern_search/#references","text":"Hooke-Jeeves Pattern Search on Wikipedia","title":"References"},{"location":"booth/","text":"Booth's Function This is a simple 2D quadratic function with a minimum of zero at [1, 3]. Form of the function is as follows: f(x, y) = (x + 2y - 7)^2 + (2x + y - 5)^2 function optiseek.testfunctions. booth ( x1, x2 ) Parameters Parameter Description x1 : float Input value for the first dimension. x2 : float Input value for the second dimension. Example from optiseek.testfunctions import booth y = booth(1, 3) References List of Test Functions on Wikipedia","title":"Booths Function"},{"location":"booth/#booths-function","text":"This is a simple 2D quadratic function with a minimum of zero at [1, 3]. Form of the function is as follows: f(x, y) = (x + 2y - 7)^2 + (2x + y - 5)^2 function optiseek.testfunctions. booth ( x1, x2 )","title":"Booth's Function"},{"location":"booth/#parameters","text":"Parameter Description x1 : float Input value for the first dimension. x2 : float Input value for the second dimension.","title":"Parameters"},{"location":"booth/#example","text":"from optiseek.testfunctions import booth y = booth(1, 3)","title":"Example"},{"location":"booth/#references","text":"List of Test Functions on Wikipedia","title":"References"},{"location":"cyclic_coordinate_descent/","text":"Cyclic Coordinate Descent This class represents a cyclic coordinate descent algorithm. This is a basic black-box optimization function that requires no knowledge of the form of the function to be optimized. The algorithm cycles through each of the dimensions in sequence and does an individual line search (a golden section search) within the maximum step size specified by the user. While the line search is executed in a certain dimension, the position values in all other dimensions are held constant. This is a deterministic method that is susceptible to getting stuck in local optima. In some cases, the algorithm gets stuck in a loop before it even reaches a local optimum. In these cases, changing the initial guess can rectify the issue. class optiseek.metaheuristics. cyclic_coordinate_descent ( input_function, initial_guess, find_minimum=True, max_iter=100, sol_threshold=None, store_results=False, max_step_size=1.0 ) Parameters Parameter Description input_function : function Function that the algorithm will use to search for an optimum. *args will be passed to the function within the solver. initial_guess : float, list of floats, or ndarray The initial guess that the algorithm will start the search from. Can be a float if the function is one-dimensional. find_minimum : bool Indicates whether the optimimum of interest is a minimum or maximum. If true, looks for minimum. If false, looks for maximum. max_iter : int Maximum number of iterations. If reached, the algorithm terminates. sol_threshold : float If a solution is found better than this threshold, the iterations stop. None indicates that the algorithm will not consider this. store_results : bool Choose whether to save intermediate iteration results for post-processing or not. If true, results will be saved. max_step_size : float Maximum step size that the algorithm can possibly take for each iteration in each direction. Attributes Attribute Description best_position : ndarray Most optimal position found during the solution iterations. best_value : float Most optimal function value found during the solution iterations. completed_iter : int Number of iterations completed during the solution process. stored_positions : ndarray Positions for each member of the population for each iteration after the solver is finished. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, position in each dimension] stored_values : ndarray Function values for each member of the population for each iteration. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, function value] Methods .solve() Executes the algorithm solution with the current parameters. Results will be stored to the class attributes. If the user opted to store intermediate results, these will also be stored. Parameters None Returns None Example from optiseek.direct import cyclic_coordinate_descent from optiseek.testfunctions import booth # create an instance of the algorithm, set its parameters, and solve alg = cyclic_coordinate_descent(booth, [0, 0]) # create instance with booth test function and initial guess [0, 0] alg.max_iter = 100 # set iteration limit alg.sol_threshold = 0.001 # set a solution threshold alg.max_step_size = 0.5 # define maximum step size # execute the algorithm alg.solve() # show the results! print(alg.best_value) print(alg.best_position) print(alg.completed_iter) References Coordinate Descent on Wikipedia","title":"Cyclic Coordinate Descent"},{"location":"cyclic_coordinate_descent/#cyclic-coordinate-descent","text":"This class represents a cyclic coordinate descent algorithm. This is a basic black-box optimization function that requires no knowledge of the form of the function to be optimized. The algorithm cycles through each of the dimensions in sequence and does an individual line search (a golden section search) within the maximum step size specified by the user. While the line search is executed in a certain dimension, the position values in all other dimensions are held constant. This is a deterministic method that is susceptible to getting stuck in local optima. In some cases, the algorithm gets stuck in a loop before it even reaches a local optimum. In these cases, changing the initial guess can rectify the issue. class optiseek.metaheuristics. cyclic_coordinate_descent ( input_function, initial_guess, find_minimum=True, max_iter=100, sol_threshold=None, store_results=False, max_step_size=1.0 )","title":"Cyclic Coordinate Descent"},{"location":"cyclic_coordinate_descent/#parameters","text":"Parameter Description input_function : function Function that the algorithm will use to search for an optimum. *args will be passed to the function within the solver. initial_guess : float, list of floats, or ndarray The initial guess that the algorithm will start the search from. Can be a float if the function is one-dimensional. find_minimum : bool Indicates whether the optimimum of interest is a minimum or maximum. If true, looks for minimum. If false, looks for maximum. max_iter : int Maximum number of iterations. If reached, the algorithm terminates. sol_threshold : float If a solution is found better than this threshold, the iterations stop. None indicates that the algorithm will not consider this. store_results : bool Choose whether to save intermediate iteration results for post-processing or not. If true, results will be saved. max_step_size : float Maximum step size that the algorithm can possibly take for each iteration in each direction.","title":"Parameters"},{"location":"cyclic_coordinate_descent/#attributes","text":"Attribute Description best_position : ndarray Most optimal position found during the solution iterations. best_value : float Most optimal function value found during the solution iterations. completed_iter : int Number of iterations completed during the solution process. stored_positions : ndarray Positions for each member of the population for each iteration after the solver is finished. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, position in each dimension] stored_values : ndarray Function values for each member of the population for each iteration. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, function value]","title":"Attributes"},{"location":"cyclic_coordinate_descent/#methods","text":".solve() Executes the algorithm solution with the current parameters. Results will be stored to the class attributes. If the user opted to store intermediate results, these will also be stored. Parameters None Returns None","title":"Methods"},{"location":"cyclic_coordinate_descent/#example","text":"from optiseek.direct import cyclic_coordinate_descent from optiseek.testfunctions import booth # create an instance of the algorithm, set its parameters, and solve alg = cyclic_coordinate_descent(booth, [0, 0]) # create instance with booth test function and initial guess [0, 0] alg.max_iter = 100 # set iteration limit alg.sol_threshold = 0.001 # set a solution threshold alg.max_step_size = 0.5 # define maximum step size # execute the algorithm alg.solve() # show the results! print(alg.best_value) print(alg.best_position) print(alg.completed_iter)","title":"Example"},{"location":"cyclic_coordinate_descent/#references","text":"Coordinate Descent on Wikipedia","title":"References"},{"location":"differential_evolution/","text":"Differential Evolution This class represents the differential evolution algorithm developed by Storn and Price. This is an evolutionary algorithm that utilizes vector-based genetic crossovers. It contains the typical components of a genetic algorithm (mutation, crossover, & selection) but has a special unique form of crossover that makes it widely applicable to a diverse set of problems. There are also very few parameters, making it simple to tune. class optiseek.metaheuristics. differential_evolution ( input_function, b_lower=-10, b_upper=10, find_minimum=True, max_iter=100, sol_threshold=None, max_unchanged_iter=None, store_results=False, n_agents=50, weight=0.2, p_crossover=0.5 ) Parameters Parameter Description input_function : function Function that the algorithm will use to search for an optimum. *args will be passed to the function within the solver. b_lower : float, list of floats, or ndarray Contains the lower bounds of each dimension in the search space. Can be a float if the function is one-dimensional. b_upper : float, list of floats, or ndarray Contains the upper bounds of each dimension in the search space. Can be a float if the function is one-dimensional. find_minimum : bool Indicates whether the optimimum of interest is a minimum or maximum. If true, looks for minimum. If false, looks for maximum. max_iter : int Maximum number of iterations. If reached, the algorithm terminates. sol_threshold : float If a solution is found better than this threshold, the iterations stop. None indicates that the algorithm will not consider this. max_unchanged_iter : int If the solution does not improve after this many iterations, the solver terminates. None indicates that the algorithm will not consider this. store_results : bool Choose whether to save intermediate iteration results for post-processing or not. If true, results will be saved. n_agents : int Number of agents to use in the population. weight : float Differential weight coefficient in [0, 2]. p_crossover : float Probability in [0, 1] that a gene crossover will occur for each dimension. Attributes Attribute Description best_position : ndarray Most optimal position found during the solution iterations. best_value : float Most optimal function value found during the solution iterations. completed_iter : int Number of iterations completed during the solution process. stored_positions : ndarray Positions for each member of the population for each iteration after the solver is finished. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, position in each dimension] stored_values : ndarray Function values for each member of the population for each iteration. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, function value] Methods .solve() Executes the algorithm solution with the current parameters. Results will be stored to the class attributes. If the user opted to store intermediate results, these will also be stored. Parameters None Returns None Example from optiseek.metaheuristics import differential_evolution from optiseek.testfunctions import booth # create an instance of the algorithm, set its parameters, and solve alg = differential_evolution(booth) # create instance with booth test function alg.b_lower = [-10, -10] # define lower bounds alg.b_upper = [10, 10] # define upper bounds alg.max_iter = 100 # set iteration limit alg.sol_threshold = 0.001 # set a solution threshold alg.n_agents = 20 # define population size alg.weight = 0.2 # set differential weight alg.p_crossover = 0.35 # set crossover probability # execute the algorithm alg.solve() # show the results! print(alg.best_value) print(alg.best_position) print(alg.completed_iter) References Differential Evolution on Wikipedia","title":"Differential Evolution"},{"location":"differential_evolution/#differential-evolution","text":"This class represents the differential evolution algorithm developed by Storn and Price. This is an evolutionary algorithm that utilizes vector-based genetic crossovers. It contains the typical components of a genetic algorithm (mutation, crossover, & selection) but has a special unique form of crossover that makes it widely applicable to a diverse set of problems. There are also very few parameters, making it simple to tune. class optiseek.metaheuristics. differential_evolution ( input_function, b_lower=-10, b_upper=10, find_minimum=True, max_iter=100, sol_threshold=None, max_unchanged_iter=None, store_results=False, n_agents=50, weight=0.2, p_crossover=0.5 )","title":"Differential Evolution"},{"location":"differential_evolution/#parameters","text":"Parameter Description input_function : function Function that the algorithm will use to search for an optimum. *args will be passed to the function within the solver. b_lower : float, list of floats, or ndarray Contains the lower bounds of each dimension in the search space. Can be a float if the function is one-dimensional. b_upper : float, list of floats, or ndarray Contains the upper bounds of each dimension in the search space. Can be a float if the function is one-dimensional. find_minimum : bool Indicates whether the optimimum of interest is a minimum or maximum. If true, looks for minimum. If false, looks for maximum. max_iter : int Maximum number of iterations. If reached, the algorithm terminates. sol_threshold : float If a solution is found better than this threshold, the iterations stop. None indicates that the algorithm will not consider this. max_unchanged_iter : int If the solution does not improve after this many iterations, the solver terminates. None indicates that the algorithm will not consider this. store_results : bool Choose whether to save intermediate iteration results for post-processing or not. If true, results will be saved. n_agents : int Number of agents to use in the population. weight : float Differential weight coefficient in [0, 2]. p_crossover : float Probability in [0, 1] that a gene crossover will occur for each dimension.","title":"Parameters"},{"location":"differential_evolution/#attributes","text":"Attribute Description best_position : ndarray Most optimal position found during the solution iterations. best_value : float Most optimal function value found during the solution iterations. completed_iter : int Number of iterations completed during the solution process. stored_positions : ndarray Positions for each member of the population for each iteration after the solver is finished. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, position in each dimension] stored_values : ndarray Function values for each member of the population for each iteration. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, function value]","title":"Attributes"},{"location":"differential_evolution/#methods","text":".solve() Executes the algorithm solution with the current parameters. Results will be stored to the class attributes. If the user opted to store intermediate results, these will also be stored. Parameters None Returns None","title":"Methods"},{"location":"differential_evolution/#example","text":"from optiseek.metaheuristics import differential_evolution from optiseek.testfunctions import booth # create an instance of the algorithm, set its parameters, and solve alg = differential_evolution(booth) # create instance with booth test function alg.b_lower = [-10, -10] # define lower bounds alg.b_upper = [10, 10] # define upper bounds alg.max_iter = 100 # set iteration limit alg.sol_threshold = 0.001 # set a solution threshold alg.n_agents = 20 # define population size alg.weight = 0.2 # set differential weight alg.p_crossover = 0.35 # set crossover probability # execute the algorithm alg.solve() # show the results! print(alg.best_value) print(alg.best_position) print(alg.completed_iter)","title":"Example"},{"location":"differential_evolution/#references","text":"Differential Evolution on Wikipedia","title":"References"},{"location":"enhanced_pattern_search/","text":"Enhanced Pattern Search This class represents an enhanced Hooke-Jeeves pattern search algorithm. This is a basic black-box optimization function that requires no knowledge of the form of the function to be optimized. It is very similar to the standard Hooke-Jeeves Pattern Search algorithm with a slight variation. Rather than taking two sample points in each dimension for each iteration, a single sample point is taken for each dimension in the positive direction with the magnitude of the step size. Then, a final sample point is taken in the negative direction of the step size in all dimensions at once. This results in a positive spanning set of search directions, with only n+1 sample points taken at each iteration. This can be more efficient than the basic Hooke-Jeeves method. class optiseek.metaheuristics. enhanced_pattern_search ( input_function, initial_guess, find_minimum=True, max_iter=100, sol_threshold=None, store_results=False, max_step_size=1.0 ) Parameters Parameter Description input_function : function Function that the algorithm will use to search for an optimum. *args will be passed to the function within the solver. initial_guess : float, list of floats, or ndarray The initial guess that the algorithm will start the search from. Can be a float if the function is one-dimensional. find_minimum : bool Indicates whether the optimimum of interest is a minimum or maximum. If true, looks for minimum. If false, looks for maximum. max_iter : int Maximum number of iterations. If reached, the algorithm terminates. sol_threshold : float If a solution is found better than this threshold, the iterations stop. None indicates that the algorithm will not consider this. store_results : bool Choose whether to save intermediate iteration results for post-processing or not. If true, results will be saved. max_step_size : float Maximum step size that the algorithm can possibly take for each iteration in each direction. Attributes Attribute Description best_position : ndarray Most optimal position found during the solution iterations. best_value : float Most optimal function value found during the solution iterations. completed_iter : int Number of iterations completed during the solution process. stored_positions : ndarray Positions for each member of the population for each iteration after the solver is finished. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, position in each dimension] stored_values : ndarray Function values for each member of the population for each iteration. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, function value] Methods .solve() Executes the algorithm solution with the current parameters. Results will be stored to the class attributes. If the user opted to store intermediate results, these will also be stored. Parameters None Returns None Example from optiseek.direct import enhanced_pattern_search from optiseek.testfunctions import booth # create an instance of the algorithm, set its parameters, and solve alg = enhanced_pattern_search(booth, [0, 0]) # create instance with booth test function and initial guess [0, 0] alg.max_iter = 100 # set iteration limit alg.sol_threshold = 0.001 # set a solution threshold alg.max_step_size = 0.5 # define maximum step size # execute the algorithm alg.solve() # show the results! print(alg.best_value) print(alg.best_position) print(alg.completed_iter) References Hooke-Jeeves Pattern Search on Wikipedia","title":"Enhanced Pattern Search"},{"location":"enhanced_pattern_search/#enhanced-pattern-search","text":"This class represents an enhanced Hooke-Jeeves pattern search algorithm. This is a basic black-box optimization function that requires no knowledge of the form of the function to be optimized. It is very similar to the standard Hooke-Jeeves Pattern Search algorithm with a slight variation. Rather than taking two sample points in each dimension for each iteration, a single sample point is taken for each dimension in the positive direction with the magnitude of the step size. Then, a final sample point is taken in the negative direction of the step size in all dimensions at once. This results in a positive spanning set of search directions, with only n+1 sample points taken at each iteration. This can be more efficient than the basic Hooke-Jeeves method. class optiseek.metaheuristics. enhanced_pattern_search ( input_function, initial_guess, find_minimum=True, max_iter=100, sol_threshold=None, store_results=False, max_step_size=1.0 )","title":"Enhanced Pattern Search"},{"location":"enhanced_pattern_search/#parameters","text":"Parameter Description input_function : function Function that the algorithm will use to search for an optimum. *args will be passed to the function within the solver. initial_guess : float, list of floats, or ndarray The initial guess that the algorithm will start the search from. Can be a float if the function is one-dimensional. find_minimum : bool Indicates whether the optimimum of interest is a minimum or maximum. If true, looks for minimum. If false, looks for maximum. max_iter : int Maximum number of iterations. If reached, the algorithm terminates. sol_threshold : float If a solution is found better than this threshold, the iterations stop. None indicates that the algorithm will not consider this. store_results : bool Choose whether to save intermediate iteration results for post-processing or not. If true, results will be saved. max_step_size : float Maximum step size that the algorithm can possibly take for each iteration in each direction.","title":"Parameters"},{"location":"enhanced_pattern_search/#attributes","text":"Attribute Description best_position : ndarray Most optimal position found during the solution iterations. best_value : float Most optimal function value found during the solution iterations. completed_iter : int Number of iterations completed during the solution process. stored_positions : ndarray Positions for each member of the population for each iteration after the solver is finished. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, position in each dimension] stored_values : ndarray Function values for each member of the population for each iteration. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, function value]","title":"Attributes"},{"location":"enhanced_pattern_search/#methods","text":".solve() Executes the algorithm solution with the current parameters. Results will be stored to the class attributes. If the user opted to store intermediate results, these will also be stored. Parameters None Returns None","title":"Methods"},{"location":"enhanced_pattern_search/#example","text":"from optiseek.direct import enhanced_pattern_search from optiseek.testfunctions import booth # create an instance of the algorithm, set its parameters, and solve alg = enhanced_pattern_search(booth, [0, 0]) # create instance with booth test function and initial guess [0, 0] alg.max_iter = 100 # set iteration limit alg.sol_threshold = 0.001 # set a solution threshold alg.max_step_size = 0.5 # define maximum step size # execute the algorithm alg.solve() # show the results! print(alg.best_value) print(alg.best_position) print(alg.completed_iter)","title":"Example"},{"location":"enhanced_pattern_search/#references","text":"Hooke-Jeeves Pattern Search on Wikipedia","title":"References"},{"location":"firefly_algorithm/","text":"Firefly Algorithm This class represents the firefly algorithm developed by Xin-She Yang. This algorithm is based on the flashing patterns and swarm behavior of fireflies. Fireflies are attracted to others based on their proximity in the search space and the brightness (function values) of others. Their movements also have a stochastic component. class optiseek.metaheuristics. firefly_algorithm ( input_function, b_lower=-10, b_upper=10, find_minimum=True, max_iter=100, sol_threshold=None, max_unchanged_iter=None, store_results=False, n_fireflies=50, beta=1.0, alpha=0.01, gamma=1.0 ) Parameters Parameter Description input_function : function Function that the algorithm will use to search for an optimum. *args will be passed to the function within the solver. b_lower : float, list of floats, or ndarray Contains the lower bounds of each dimension in the search space. Can be a float if the function is one-dimensional. b_upper : float, list of floats, or ndarray Contains the upper bounds of each dimension in the search space. Can be a float if the function is one-dimensional. find_minimum : bool Indicates whether the optimimum of interest is a minimum or maximum. If true, looks for minimum. If false, looks for maximum. max_iter : int Maximum number of iterations. If reached, the algorithm terminates. sol_threshold : float If a solution is found better than this threshold, the iterations stop. None indicates that the algorithm will not consider this. max_unchanged_iter : int If the solution does not improve after this many iterations, the solver terminates. None indicates that the algorithm will not consider this. store_results : bool Choose whether to save intermediate iteration results for post-processing or not. If true, results will be saved. n_fireflies : int Number of fireflies to use in the swarm population. beta : float Linear visibility coefficient in [0.1, 1.5]. Lower value indicates that the fireflies are less attracted to each other. alpha : float Coefficient in [0, 0.1] that is a portion of each dimension's bound widths to use for the random walk. gamma : float Exponential visibility coefficient in [0.01, 1]. Higher value indicates that the fireflies are less attracted to each other. Attributes Attribute Description best_position : ndarray Most optimal position found during the solution iterations. best_value : float Most optimal function value found during the solution iterations. completed_iter : int Number of iterations completed during the solution process. stored_positions : ndarray Positions for each member of the population for each iteration after the solver is finished. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, position in each dimension] stored_values : ndarray Function values for each member of the population for each iteration. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, function value] Methods .solve() Executes the algorithm solution with the current parameters. Results will be stored to the class attributes. If the user opted to store intermediate results, these will also be stored. Parameters None Returns None Example from optiseek.metaheuristics import firefly_algorithm from optiseek.testfunctions import booth # create an instance of the algorithm, set its parameters, and solve alg = firefly_algorithm(booth) # create instance with booth test function alg.b_lower = [-10, -10] # define lower bounds alg.b_upper = [10, 10] # define upper bounds alg.max_iter = 100 # set iteration limit alg.sol_threshold = 0.001 # set a solution threshold alg.n_fireflies = 20 # define population size alg.beta = 0.3 # set linear visibility coefficient alg.alpha = 0.5 # set random walk coefficient alg.gamma = 1.5 # set exponential visibility coefficient # execute the algorithm alg.solve() # show the results! print(alg.best_value) print(alg.best_position) print(alg.completed_iter) References Firefly Algorithm on Wikipedia","title":"Firefly Algorithm"},{"location":"firefly_algorithm/#firefly-algorithm","text":"This class represents the firefly algorithm developed by Xin-She Yang. This algorithm is based on the flashing patterns and swarm behavior of fireflies. Fireflies are attracted to others based on their proximity in the search space and the brightness (function values) of others. Their movements also have a stochastic component. class optiseek.metaheuristics. firefly_algorithm ( input_function, b_lower=-10, b_upper=10, find_minimum=True, max_iter=100, sol_threshold=None, max_unchanged_iter=None, store_results=False, n_fireflies=50, beta=1.0, alpha=0.01, gamma=1.0 )","title":"Firefly Algorithm"},{"location":"firefly_algorithm/#parameters","text":"Parameter Description input_function : function Function that the algorithm will use to search for an optimum. *args will be passed to the function within the solver. b_lower : float, list of floats, or ndarray Contains the lower bounds of each dimension in the search space. Can be a float if the function is one-dimensional. b_upper : float, list of floats, or ndarray Contains the upper bounds of each dimension in the search space. Can be a float if the function is one-dimensional. find_minimum : bool Indicates whether the optimimum of interest is a minimum or maximum. If true, looks for minimum. If false, looks for maximum. max_iter : int Maximum number of iterations. If reached, the algorithm terminates. sol_threshold : float If a solution is found better than this threshold, the iterations stop. None indicates that the algorithm will not consider this. max_unchanged_iter : int If the solution does not improve after this many iterations, the solver terminates. None indicates that the algorithm will not consider this. store_results : bool Choose whether to save intermediate iteration results for post-processing or not. If true, results will be saved. n_fireflies : int Number of fireflies to use in the swarm population. beta : float Linear visibility coefficient in [0.1, 1.5]. Lower value indicates that the fireflies are less attracted to each other. alpha : float Coefficient in [0, 0.1] that is a portion of each dimension's bound widths to use for the random walk. gamma : float Exponential visibility coefficient in [0.01, 1]. Higher value indicates that the fireflies are less attracted to each other.","title":"Parameters"},{"location":"firefly_algorithm/#attributes","text":"Attribute Description best_position : ndarray Most optimal position found during the solution iterations. best_value : float Most optimal function value found during the solution iterations. completed_iter : int Number of iterations completed during the solution process. stored_positions : ndarray Positions for each member of the population for each iteration after the solver is finished. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, position in each dimension] stored_values : ndarray Function values for each member of the population for each iteration. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, function value]","title":"Attributes"},{"location":"firefly_algorithm/#methods","text":".solve() Executes the algorithm solution with the current parameters. Results will be stored to the class attributes. If the user opted to store intermediate results, these will also be stored. Parameters None Returns None","title":"Methods"},{"location":"firefly_algorithm/#example","text":"from optiseek.metaheuristics import firefly_algorithm from optiseek.testfunctions import booth # create an instance of the algorithm, set its parameters, and solve alg = firefly_algorithm(booth) # create instance with booth test function alg.b_lower = [-10, -10] # define lower bounds alg.b_upper = [10, 10] # define upper bounds alg.max_iter = 100 # set iteration limit alg.sol_threshold = 0.001 # set a solution threshold alg.n_fireflies = 20 # define population size alg.beta = 0.3 # set linear visibility coefficient alg.alpha = 0.5 # set random walk coefficient alg.gamma = 1.5 # set exponential visibility coefficient # execute the algorithm alg.solve() # show the results! print(alg.best_value) print(alg.best_position) print(alg.completed_iter)","title":"Example"},{"location":"firefly_algorithm/#references","text":"Firefly Algorithm on Wikipedia","title":"References"},{"location":"mayfly_algorithm/","text":"Mayfly Algorithm This class represents the mayfly algorithm developed by Zervoudakis and Tsafarakis. This algorithm takes components from swarm-based algorithms as well as genetic algorithms and combines them into a powerful hybrid algorithm based on the mating behavior of mayflies. An initial population is split into males and females, each moving in different ways. The males exhibit swarm behavior to gather towards the best male (at the best function value), similar to particle swarm optimization. The females are attracted to a matched male if the male has a better function value. In each iteration, there is a genetic crossover between the males and females and selection of the best in the population takes place. Stochastic components are introduced into the movements to avoid local optima. This is a very powerful algorithm, but requires many parameters. class optiseek.metaheuristics. mayfly_algorithm ( input_function, b_lower=-10, b_upper=10, find_minimum=True, max_iter=100, sol_threshold=None, max_unchanged_iter=None, store_results=False, n_mayflies=50, beta=0.7, gravity=0.6, alpha_cog=0.5, alpha_soc=1.5, alpha_attract=1.5, nuptial_coeff=0.05 ) Parameters Parameter Description input_function : function Function that the algorithm will use to search for an optimum. *args will be passed to the function within the solver. b_lower : float, list of floats, or ndarray Contains the lower bounds of each dimension in the search space. Can be a float if the function is one-dimensional. b_upper : float, list of floats, or ndarray Contains the upper bounds of each dimension in the search space. Can be a float if the function is one-dimensional. find_minimum : bool Indicates whether the optimimum of interest is a minimum or maximum. If true, looks for minimum. If false, looks for maximum. max_iter : int Maximum number of iterations. If reached, the algorithm terminates. sol_threshold : float If a solution is found better than this threshold, the iterations stop. None indicates that the algorithm will not consider this. max_unchanged_iter : int If the solution does not improve after this many iterations, the solver terminates. None indicates that the algorithm will not consider this. store_results : bool Choose whether to save intermediate iteration results for post-processing or not. If true, results will be saved. n_mayflies : int Number of mayflies to use in the population. beta : float Exponential visibility coefficient in [0.1, 1]. Higher value means that mayflies are less drawn towards others. gravity : float Gravity coefficient in [0.1, 1]. Lower value means that the mayflies have less momentum. alpha_cog : float Cognitive coefficient in [0, 2]. Indicates how attracted the male mayflies are to their individually best known position. alpha_soc : float Social coefficient in [0, 2]. Indicates how attracted the male mayflies are to the male swarm's best known position. alpha_attract : float Attraction coefficient in [0, 2]. Indicates how attracted the females are to their matched male counterpart. nuptial_coeff : float Nuptial coefficient in [0, 0.4], a multiplier on bound widths for each dimension used for the male and female random walks. Attributes Attribute Description best_position : ndarray Most optimal position found during the solution iterations. best_value : float Most optimal function value found during the solution iterations. completed_iter : int Number of iterations completed during the solution process. stored_positions : ndarray Positions for each member of the population for each iteration after the solver is finished. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, position in each dimension] stored_values : ndarray Function values for each member of the population for each iteration. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, function value] Methods .solve() Executes the algorithm solution with the current parameters. Results will be stored to the class attributes. If the user opted to store intermediate results, these will also be stored. Parameters None Returns None Example from optiseek.metaheuristics import mayfly_algorithm from optiseek.testfunctions import booth # create an instance of the algorithm, set its parameters, and solve alg = mayfly_algorithm(booth) # create instance with booth test function alg.b_lower = [-10, -10] # define lower bounds alg.b_upper = [10, 10] # define upper bounds alg.max_iter = 100 # set iteration limit alg.sol_threshold = 0.001 # set a solution threshold alg.n_mayflies = 20 # set mayfly population alg.beta = 0.5 # set visibility coefficient alg.gravity = 0.4 # set gravity coefficient alg.alpha_cog = 0.5 # set male cognitive coefficient alg.alpha_soc = 1.5 # set male social coefficient alg.alpha_attract = 1.0 # set female attraction coefficient alg.nuptial_coeff = 0.02 # set random walk coefficient # execute the algorithm alg.solve() # show the results! print(alg.best_value) print(alg.best_position) print(alg.completed_iter) References A mayfly optimization algorithm, by Konstantinos and Tsafarakis","title":"Mayfly Algorithm"},{"location":"mayfly_algorithm/#mayfly-algorithm","text":"This class represents the mayfly algorithm developed by Zervoudakis and Tsafarakis. This algorithm takes components from swarm-based algorithms as well as genetic algorithms and combines them into a powerful hybrid algorithm based on the mating behavior of mayflies. An initial population is split into males and females, each moving in different ways. The males exhibit swarm behavior to gather towards the best male (at the best function value), similar to particle swarm optimization. The females are attracted to a matched male if the male has a better function value. In each iteration, there is a genetic crossover between the males and females and selection of the best in the population takes place. Stochastic components are introduced into the movements to avoid local optima. This is a very powerful algorithm, but requires many parameters. class optiseek.metaheuristics. mayfly_algorithm ( input_function, b_lower=-10, b_upper=10, find_minimum=True, max_iter=100, sol_threshold=None, max_unchanged_iter=None, store_results=False, n_mayflies=50, beta=0.7, gravity=0.6, alpha_cog=0.5, alpha_soc=1.5, alpha_attract=1.5, nuptial_coeff=0.05 )","title":"Mayfly Algorithm"},{"location":"mayfly_algorithm/#parameters","text":"Parameter Description input_function : function Function that the algorithm will use to search for an optimum. *args will be passed to the function within the solver. b_lower : float, list of floats, or ndarray Contains the lower bounds of each dimension in the search space. Can be a float if the function is one-dimensional. b_upper : float, list of floats, or ndarray Contains the upper bounds of each dimension in the search space. Can be a float if the function is one-dimensional. find_minimum : bool Indicates whether the optimimum of interest is a minimum or maximum. If true, looks for minimum. If false, looks for maximum. max_iter : int Maximum number of iterations. If reached, the algorithm terminates. sol_threshold : float If a solution is found better than this threshold, the iterations stop. None indicates that the algorithm will not consider this. max_unchanged_iter : int If the solution does not improve after this many iterations, the solver terminates. None indicates that the algorithm will not consider this. store_results : bool Choose whether to save intermediate iteration results for post-processing or not. If true, results will be saved. n_mayflies : int Number of mayflies to use in the population. beta : float Exponential visibility coefficient in [0.1, 1]. Higher value means that mayflies are less drawn towards others. gravity : float Gravity coefficient in [0.1, 1]. Lower value means that the mayflies have less momentum. alpha_cog : float Cognitive coefficient in [0, 2]. Indicates how attracted the male mayflies are to their individually best known position. alpha_soc : float Social coefficient in [0, 2]. Indicates how attracted the male mayflies are to the male swarm's best known position. alpha_attract : float Attraction coefficient in [0, 2]. Indicates how attracted the females are to their matched male counterpart. nuptial_coeff : float Nuptial coefficient in [0, 0.4], a multiplier on bound widths for each dimension used for the male and female random walks.","title":"Parameters"},{"location":"mayfly_algorithm/#attributes","text":"Attribute Description best_position : ndarray Most optimal position found during the solution iterations. best_value : float Most optimal function value found during the solution iterations. completed_iter : int Number of iterations completed during the solution process. stored_positions : ndarray Positions for each member of the population for each iteration after the solver is finished. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, position in each dimension] stored_values : ndarray Function values for each member of the population for each iteration. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, function value]","title":"Attributes"},{"location":"mayfly_algorithm/#methods","text":".solve() Executes the algorithm solution with the current parameters. Results will be stored to the class attributes. If the user opted to store intermediate results, these will also be stored. Parameters None Returns None","title":"Methods"},{"location":"mayfly_algorithm/#example","text":"from optiseek.metaheuristics import mayfly_algorithm from optiseek.testfunctions import booth # create an instance of the algorithm, set its parameters, and solve alg = mayfly_algorithm(booth) # create instance with booth test function alg.b_lower = [-10, -10] # define lower bounds alg.b_upper = [10, 10] # define upper bounds alg.max_iter = 100 # set iteration limit alg.sol_threshold = 0.001 # set a solution threshold alg.n_mayflies = 20 # set mayfly population alg.beta = 0.5 # set visibility coefficient alg.gravity = 0.4 # set gravity coefficient alg.alpha_cog = 0.5 # set male cognitive coefficient alg.alpha_soc = 1.5 # set male social coefficient alg.alpha_attract = 1.0 # set female attraction coefficient alg.nuptial_coeff = 0.02 # set random walk coefficient # execute the algorithm alg.solve() # show the results! print(alg.best_value) print(alg.best_position) print(alg.completed_iter)","title":"Example"},{"location":"mayfly_algorithm/#references","text":"A mayfly optimization algorithm, by Konstantinos and Tsafarakis","title":"References"},{"location":"particle_swarm_optimization/","text":"Particle Swarm Optimizer This class represents a standard particle swarm optimization algorithm, originally developed by Kennedy and Eberhart. This algorithm is based on swarm behavior commonly observed in nature. A population of particles is introduced to traverse the search space. Their movement is influenced by their own previous positions, the best known position of the swarm, and some stochastic velocity. class optiseek.metaheuristics. particle_swarm_optimizer ( input_function, b_lower=-10, b_upper=10, find_minimum=True, max_iter=100, sol_threshold=None, max_unchanged_iter=None, store_results=False, n_particles=50, weight=0.25, phi_p=1, phi_g=2, zero_velocity=True ) Parameters Parameter Description input_function : function Function that the algorithm will use to search for an optimum. *args will be passed to the function within the solver. b_lower : float, list of floats, or ndarray Contains the lower bounds of each dimension in the search space. Can be a float if the function is one-dimensional. b_upper : float, list of floats, or ndarray Contains the upper bounds of each dimension in the search space. Can be a float if the function is one-dimensional. find_minimum : bool Indicates whether the optimimum of interest is a minimum or maximum. If true, looks for minimum. If false, looks for maximum. max_iter : int Maximum number of iterations. If reached, the algorithm terminates. sol_threshold : float If a solution is found better than this threshold, the iterations stop. None indicates that the algorithm will not consider this. max_unchanged_iter : int If the solution does not improve after this many iterations, the solver terminates. None indicates that the algorithm will not consider this. store_results : bool Choose whether to save intermediate iteration results for post-processing or not. If true, results will be saved. n_particles : int Number of particles to use in the particle swarm population. weight : float Weight coefficient in [0, 1]. Lower weight gives the particles less momentum. phi_p : float Cognitive coefficient in [0, 3]. Higher value indicates that the particles are drawn more towards their own best known position. phi_g : float Social coefficient in [0, 3]. Higher value indicates that the particles are drawn more towards the swarm's collectively best known position. zero_velocity : bool Choose whether the particles start off with zero velocity or a random initial velocity. Initial velocities can sometimes cause divergence. Attributes Attribute Description best_position : ndarray Most optimal position found during the solution iterations. best_value : float Most optimal function value found during the solution iterations. completed_iter : int Number of iterations completed during the solution process. stored_positions : ndarray Positions for each member of the population for each iteration after the solver is finished. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, position in each dimension] stored_values : ndarray Function values for each member of the population for each iteration. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, function value] Methods .solve() Executes the algorithm solution with the current parameters. Results will be stored to the class attributes. If the user opted to store intermediate results, these will also be stored. Parameters None Returns None Example from optiseek.metaheuristics import particle_swarm_optimizer from optiseek.testfunctions import booth # create an instance of the algorithm, set its parameters, and solve alg = particle_swarm_optimizer(booth) # create instance with booth test function alg.b_lower = [-10, -10] # define lower bounds alg.b_upper = [10, 10] # define upper bounds alg.max_iter = 100 # set iteration limit alg.sol_threshold = 0.001 # set a solution threshold alg.n_particles = 20 # define population size alg.weight = 0.3 # set weight alg.phi_p = 0.5 # set cognitive coefficient alg.phi_g = 1.5 # set social coefficient # execute the algorithm alg.solve() # show the results! print(alg.best_value) print(alg.best_position) print(alg.completed_iter) References Particle Swarm Optimization on Wikipedia","title":"Particle Swarm Optimizer"},{"location":"particle_swarm_optimization/#particle-swarm-optimizer","text":"This class represents a standard particle swarm optimization algorithm, originally developed by Kennedy and Eberhart. This algorithm is based on swarm behavior commonly observed in nature. A population of particles is introduced to traverse the search space. Their movement is influenced by their own previous positions, the best known position of the swarm, and some stochastic velocity. class optiseek.metaheuristics. particle_swarm_optimizer ( input_function, b_lower=-10, b_upper=10, find_minimum=True, max_iter=100, sol_threshold=None, max_unchanged_iter=None, store_results=False, n_particles=50, weight=0.25, phi_p=1, phi_g=2, zero_velocity=True )","title":"Particle Swarm Optimizer"},{"location":"particle_swarm_optimization/#parameters","text":"Parameter Description input_function : function Function that the algorithm will use to search for an optimum. *args will be passed to the function within the solver. b_lower : float, list of floats, or ndarray Contains the lower bounds of each dimension in the search space. Can be a float if the function is one-dimensional. b_upper : float, list of floats, or ndarray Contains the upper bounds of each dimension in the search space. Can be a float if the function is one-dimensional. find_minimum : bool Indicates whether the optimimum of interest is a minimum or maximum. If true, looks for minimum. If false, looks for maximum. max_iter : int Maximum number of iterations. If reached, the algorithm terminates. sol_threshold : float If a solution is found better than this threshold, the iterations stop. None indicates that the algorithm will not consider this. max_unchanged_iter : int If the solution does not improve after this many iterations, the solver terminates. None indicates that the algorithm will not consider this. store_results : bool Choose whether to save intermediate iteration results for post-processing or not. If true, results will be saved. n_particles : int Number of particles to use in the particle swarm population. weight : float Weight coefficient in [0, 1]. Lower weight gives the particles less momentum. phi_p : float Cognitive coefficient in [0, 3]. Higher value indicates that the particles are drawn more towards their own best known position. phi_g : float Social coefficient in [0, 3]. Higher value indicates that the particles are drawn more towards the swarm's collectively best known position. zero_velocity : bool Choose whether the particles start off with zero velocity or a random initial velocity. Initial velocities can sometimes cause divergence.","title":"Parameters"},{"location":"particle_swarm_optimization/#attributes","text":"Attribute Description best_position : ndarray Most optimal position found during the solution iterations. best_value : float Most optimal function value found during the solution iterations. completed_iter : int Number of iterations completed during the solution process. stored_positions : ndarray Positions for each member of the population for each iteration after the solver is finished. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, position in each dimension] stored_values : ndarray Function values for each member of the population for each iteration. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, function value]","title":"Attributes"},{"location":"particle_swarm_optimization/#methods","text":".solve() Executes the algorithm solution with the current parameters. Results will be stored to the class attributes. If the user opted to store intermediate results, these will also be stored. Parameters None Returns None","title":"Methods"},{"location":"particle_swarm_optimization/#example","text":"from optiseek.metaheuristics import particle_swarm_optimizer from optiseek.testfunctions import booth # create an instance of the algorithm, set its parameters, and solve alg = particle_swarm_optimizer(booth) # create instance with booth test function alg.b_lower = [-10, -10] # define lower bounds alg.b_upper = [10, 10] # define upper bounds alg.max_iter = 100 # set iteration limit alg.sol_threshold = 0.001 # set a solution threshold alg.n_particles = 20 # define population size alg.weight = 0.3 # set weight alg.phi_p = 0.5 # set cognitive coefficient alg.phi_g = 1.5 # set social coefficient # execute the algorithm alg.solve() # show the results! print(alg.best_value) print(alg.best_position) print(alg.completed_iter)","title":"Example"},{"location":"particle_swarm_optimization/#references","text":"Particle Swarm Optimization on Wikipedia","title":"References"},{"location":"rosenbrock/","text":"Rosenbrock's Function This is a 2D function with a global minimum of zero at [1, 1]. Form of the function is as follows: f(x, y) = (1 - x1)^2 + 5(x2 - x1^2)^2 function optiseek.testfunctions. rosenbrock ( x1, x2 ) Parameters Parameter Description x1 : float Input value for the first dimension. x2 : float Input value for the second dimension. Example from optiseek.testfunctions import rosenbrock y = rosenbrock(1, 1) References List of Test Functions on Wikipedia","title":"Rosenbrock Function"},{"location":"rosenbrock/#rosenbrocks-function","text":"This is a 2D function with a global minimum of zero at [1, 1]. Form of the function is as follows: f(x, y) = (1 - x1)^2 + 5(x2 - x1^2)^2 function optiseek.testfunctions. rosenbrock ( x1, x2 )","title":"Rosenbrock's Function"},{"location":"rosenbrock/#parameters","text":"Parameter Description x1 : float Input value for the first dimension. x2 : float Input value for the second dimension.","title":"Parameters"},{"location":"rosenbrock/#example","text":"from optiseek.testfunctions import rosenbrock y = rosenbrock(1, 1)","title":"Example"},{"location":"rosenbrock/#references","text":"List of Test Functions on Wikipedia","title":"References"},{"location":"simulated_annealing/","text":"Simulated Annealing This class represents the simulated annealing algorithm developed by Kirkpatrick et al. This is a local search method that takes inspiration from the annealing process in metals. Unlike deterministic gradient-based search methods, this algorithm has the ability to avoid being trapped in local optima. This is accomplished because there is a probability that a worse solution could be accepted during each iteration. As the iterations progress (i.e. temperature decreases), this probability diminishes and the algorithm is able to settle into what is hopefully a global optimum. class optiseek.metaheuristics. simulated_annealing ( input_function, b_lower=-10, b_upper=10, find_minimum=True, max_iter=100, sol_threshold=None, max_unchanged_iter=None, sigma_coeff=0.2, neighbor_dim_changes=1, initial_guess=None, store_results=False, start_temperature=10, alpha=0.9 ) Parameters Parameter Description input_function : function Function that the algorithm will use to search for an optimum. *args will be passed to the function within the solver. b_lower : float, list of floats, or ndarray Contains the lower bounds of each dimension in the search space. Can be a float if the function is one-dimensional. b_upper : float, list of floats, or ndarray Contains the upper bounds of each dimension in the search space. Can be a float if the function is one-dimensional. find_minimum : bool Indicates whether the optimimum of interest is a minimum or maximum. If true, looks for minimum. If false, looks for maximum. max_iter : int Maximum number of iterations. If reached, the algorithm terminates. sol_threshold : float If a solution is found better than this threshold, the iterations stop. None indicates that the algorithm will not consider this. max_unchanged_iter : int If the solution does not improve after this many iterations, the solver terminates. None indicates that the algorithm will not consider this. sigma_coeff : float Coefficient in (0, 0.5] to be multiplied by the bound widths for each dimension; the corresponding number is used for the standard deviation in the neighbor generation process. neighbor_dim_changes : int Number of dimensions to mutate during the generation of a new neighbor position. Must be in [1, number of dimensions] initial_guess : list of floats or ndarray Initial guess used in the solution process. Leave as None to start with a random initial guess. store_results : bool Choose whether to save intermediate iteration results for post-processing or not. If true, results will be saved. start_temperature : float Initial temperature to start iterations with. alpha : float Temperature decay coefficient in [0.6, 1). The current temperature is multiplied by this at the end of each iteration. Attributes Attribute Description best_position : ndarray Most optimal position found during the solution iterations. best_value : float Most optimal function value found during the solution iterations. completed_iter : int Number of iterations completed during the solution process. stored_positions : ndarray Positions for each member of the population for each iteration after the solver is finished. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, position in each dimension] stored_values : ndarray Function values for each member of the population for each iteration. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, function value] Methods .solve() Executes the algorithm solution with the current parameters. Results will be stored to the class attributes. If the user opted to store intermediate results, these will also be stored. Parameters None Returns None Example from optiseek.metaheuristics import simulated_annealing from optiseek.testfunctions import booth # create an instance of the algorithm, set its parameters, and solve alg = simulated_annealing(booth) # create instance with booth test function alg.b_lower = [-10, -10] # define lower bounds alg.b_upper = [10, 10] # define upper bounds alg.max_iter = 100 # set iteration limit alg.sol_threshold = 0.001 # set a solution threshold alg.sigma_coeff = 0.02 # set a multiplier of bound widths for std. dev. alg.neighbor_dim_changes = 1 # only mutate 1 dimension at a time for neighbors alg.initial_guess = [2, 5] # set an initial guess of the optimum alg.start_temperature = 5 # start the temperature at 5 alg.alpha = 0.925 # set the temperature decay rate # execute the algorithm alg.solve() # show the results! print(alg.best_value) print(alg.best_position) print(alg.completed_iter) References Simulated Annealing on Wikipedia","title":"Simulated Annealing"},{"location":"simulated_annealing/#simulated-annealing","text":"This class represents the simulated annealing algorithm developed by Kirkpatrick et al. This is a local search method that takes inspiration from the annealing process in metals. Unlike deterministic gradient-based search methods, this algorithm has the ability to avoid being trapped in local optima. This is accomplished because there is a probability that a worse solution could be accepted during each iteration. As the iterations progress (i.e. temperature decreases), this probability diminishes and the algorithm is able to settle into what is hopefully a global optimum. class optiseek.metaheuristics. simulated_annealing ( input_function, b_lower=-10, b_upper=10, find_minimum=True, max_iter=100, sol_threshold=None, max_unchanged_iter=None, sigma_coeff=0.2, neighbor_dim_changes=1, initial_guess=None, store_results=False, start_temperature=10, alpha=0.9 )","title":"Simulated Annealing"},{"location":"simulated_annealing/#parameters","text":"Parameter Description input_function : function Function that the algorithm will use to search for an optimum. *args will be passed to the function within the solver. b_lower : float, list of floats, or ndarray Contains the lower bounds of each dimension in the search space. Can be a float if the function is one-dimensional. b_upper : float, list of floats, or ndarray Contains the upper bounds of each dimension in the search space. Can be a float if the function is one-dimensional. find_minimum : bool Indicates whether the optimimum of interest is a minimum or maximum. If true, looks for minimum. If false, looks for maximum. max_iter : int Maximum number of iterations. If reached, the algorithm terminates. sol_threshold : float If a solution is found better than this threshold, the iterations stop. None indicates that the algorithm will not consider this. max_unchanged_iter : int If the solution does not improve after this many iterations, the solver terminates. None indicates that the algorithm will not consider this. sigma_coeff : float Coefficient in (0, 0.5] to be multiplied by the bound widths for each dimension; the corresponding number is used for the standard deviation in the neighbor generation process. neighbor_dim_changes : int Number of dimensions to mutate during the generation of a new neighbor position. Must be in [1, number of dimensions] initial_guess : list of floats or ndarray Initial guess used in the solution process. Leave as None to start with a random initial guess. store_results : bool Choose whether to save intermediate iteration results for post-processing or not. If true, results will be saved. start_temperature : float Initial temperature to start iterations with. alpha : float Temperature decay coefficient in [0.6, 1). The current temperature is multiplied by this at the end of each iteration.","title":"Parameters"},{"location":"simulated_annealing/#attributes","text":"Attribute Description best_position : ndarray Most optimal position found during the solution iterations. best_value : float Most optimal function value found during the solution iterations. completed_iter : int Number of iterations completed during the solution process. stored_positions : ndarray Positions for each member of the population for each iteration after the solver is finished. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, position in each dimension] stored_values : ndarray Function values for each member of the population for each iteration. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, function value]","title":"Attributes"},{"location":"simulated_annealing/#methods","text":".solve() Executes the algorithm solution with the current parameters. Results will be stored to the class attributes. If the user opted to store intermediate results, these will also be stored. Parameters None Returns None","title":"Methods"},{"location":"simulated_annealing/#example","text":"from optiseek.metaheuristics import simulated_annealing from optiseek.testfunctions import booth # create an instance of the algorithm, set its parameters, and solve alg = simulated_annealing(booth) # create instance with booth test function alg.b_lower = [-10, -10] # define lower bounds alg.b_upper = [10, 10] # define upper bounds alg.max_iter = 100 # set iteration limit alg.sol_threshold = 0.001 # set a solution threshold alg.sigma_coeff = 0.02 # set a multiplier of bound widths for std. dev. alg.neighbor_dim_changes = 1 # only mutate 1 dimension at a time for neighbors alg.initial_guess = [2, 5] # set an initial guess of the optimum alg.start_temperature = 5 # start the temperature at 5 alg.alpha = 0.925 # set the temperature decay rate # execute the algorithm alg.solve() # show the results! print(alg.best_value) print(alg.best_position) print(alg.completed_iter)","title":"Example"},{"location":"simulated_annealing/#references","text":"Simulated Annealing on Wikipedia","title":"References"},{"location":"tabu_search/","text":"Tabu Search This class represents the tabu search algorithm developed by Glover. This is a local search algorithm that utilizes memory of past positions in order to avoid getting stuck in local optima. The algorithm finds a new neighbor at each iteration; if the neighbor matches a previous position on the \"tabu list\" (within a specified threshold), then that position is prohibited unless it is better than the currently best known position. The tabu list consists of positions from a specified number of previous iterations. The length of the tabu list and tabu threshold can both be tuned. class optiseek.metaheuristics. tabu_search ( input_function, b_lower=-10, b_upper=10, find_minimum=True, max_iter=100, sol_threshold=None, max_unchanged_iter=None, sigma_coeff=0.2, neighbor_dim_changes=1, initial_guess=None, store_results=False, tenure=5, n_candidates=5, neighbor_tolerance=0.02 ) Parameters Parameter Description input_function : function Function that the algorithm will use to search for an optimum. *args will be passed to the function within the solver. b_lower : float, list of floats, or ndarray Contains the lower bounds of each dimension in the search space. Can be a float if the function is one-dimensional. b_upper : float, list of floats, or ndarray Contains the upper bounds of each dimension in the search space. Can be a float if the function is one-dimensional. find_minimum : bool Indicates whether the optimimum of interest is a minimum or maximum. If true, looks for minimum. If false, looks for maximum. max_iter : int Maximum number of iterations. If reached, the algorithm terminates. sol_threshold : float If a solution is found better than this threshold, the iterations stop. None indicates that the algorithm will not consider this. max_unchanged_iter : int If the solution does not improve after this many iterations, the solver terminates. None indicates that the algorithm will not consider this. sigma_coeff : float Coefficient in (0, 0.5] to be multiplied by the bound widths for each dimension; the corresponding number is used for the standard deviation in the neighbor generation process. neighbor_dim_changes : int Number of dimensions to mutate during the generation of a new neighbor position. Must be in [1, number of dimensions] initial_guess : list of floats or ndarray Initial guess used in the solution process. Leave as None to start with a random initial guess. store_results : bool Choose whether to save intermediate iteration results for post-processing or not. If true, results will be saved. tenure : int Number of previous positions stored on the tabu list. These positions (within a specified tolerance) will be prohibited in following iterations. n_candidates : int Number of new candidate solutions to guess at each iteration. The best solution that is not tabu is used. neighbor_tolerance : float Portion of dimension width to use as a tolerance when determining whether a potential position is tabu. Attributes Attribute Description best_position : ndarray Most optimal position found during the solution iterations. best_value : float Most optimal function value found during the solution iterations. completed_iter : int Number of iterations completed during the solution process. stored_positions : ndarray Positions for each member of the population for each iteration after the solver is finished. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, position in each dimension] stored_values : ndarray Function values for each member of the population for each iteration. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, function value] Methods .solve() Executes the algorithm solution with the current parameters. Results will be stored to the class attributes. If the user opted to store intermediate results, these will also be stored. Parameters None Returns None Example from optiseek.metaheuristics import tabu_search from optiseek.testfunctions import booth # create an instance of the algorithm, set its parameters, and solve alg = tabu_search(booth) # create instance with booth test function alg.b_lower = [-10, -10] # define lower bounds alg.b_upper = [10, 10] # define upper bounds alg.max_iter = 100 # set iteration limit alg.sol_threshold = 0.001 # set a solution threshold alg.sigma_coeff = 0.02 # set a multiplier of bound widths for std. dev. alg.neighbor_dim_changes = 1 # only mutate 1 dimension at a time for neighbors alg.initial_guess = [2, 5] # set an initial guess of the optimum alg.tenure = 7 # set the length of the tabu list to 7 previous positions alg.n_candidates = 4 # generate four neighbors during each iteration and pick best alg.neighbor_tolerance = 0.015 # fraction of bound widths to use for tolerance # execute the algorithm alg.solve() # show the results! print(alg.best_value) print(alg.best_position) print(alg.completed_iter) References Tabu Search on Wikipedia","title":"Tabu Search"},{"location":"tabu_search/#tabu-search","text":"This class represents the tabu search algorithm developed by Glover. This is a local search algorithm that utilizes memory of past positions in order to avoid getting stuck in local optima. The algorithm finds a new neighbor at each iteration; if the neighbor matches a previous position on the \"tabu list\" (within a specified threshold), then that position is prohibited unless it is better than the currently best known position. The tabu list consists of positions from a specified number of previous iterations. The length of the tabu list and tabu threshold can both be tuned. class optiseek.metaheuristics. tabu_search ( input_function, b_lower=-10, b_upper=10, find_minimum=True, max_iter=100, sol_threshold=None, max_unchanged_iter=None, sigma_coeff=0.2, neighbor_dim_changes=1, initial_guess=None, store_results=False, tenure=5, n_candidates=5, neighbor_tolerance=0.02 )","title":"Tabu Search"},{"location":"tabu_search/#parameters","text":"Parameter Description input_function : function Function that the algorithm will use to search for an optimum. *args will be passed to the function within the solver. b_lower : float, list of floats, or ndarray Contains the lower bounds of each dimension in the search space. Can be a float if the function is one-dimensional. b_upper : float, list of floats, or ndarray Contains the upper bounds of each dimension in the search space. Can be a float if the function is one-dimensional. find_minimum : bool Indicates whether the optimimum of interest is a minimum or maximum. If true, looks for minimum. If false, looks for maximum. max_iter : int Maximum number of iterations. If reached, the algorithm terminates. sol_threshold : float If a solution is found better than this threshold, the iterations stop. None indicates that the algorithm will not consider this. max_unchanged_iter : int If the solution does not improve after this many iterations, the solver terminates. None indicates that the algorithm will not consider this. sigma_coeff : float Coefficient in (0, 0.5] to be multiplied by the bound widths for each dimension; the corresponding number is used for the standard deviation in the neighbor generation process. neighbor_dim_changes : int Number of dimensions to mutate during the generation of a new neighbor position. Must be in [1, number of dimensions] initial_guess : list of floats or ndarray Initial guess used in the solution process. Leave as None to start with a random initial guess. store_results : bool Choose whether to save intermediate iteration results for post-processing or not. If true, results will be saved. tenure : int Number of previous positions stored on the tabu list. These positions (within a specified tolerance) will be prohibited in following iterations. n_candidates : int Number of new candidate solutions to guess at each iteration. The best solution that is not tabu is used. neighbor_tolerance : float Portion of dimension width to use as a tolerance when determining whether a potential position is tabu.","title":"Parameters"},{"location":"tabu_search/#attributes","text":"Attribute Description best_position : ndarray Most optimal position found during the solution iterations. best_value : float Most optimal function value found during the solution iterations. completed_iter : int Number of iterations completed during the solution process. stored_positions : ndarray Positions for each member of the population for each iteration after the solver is finished. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, position in each dimension] stored_values : ndarray Function values for each member of the population for each iteration. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, function value]","title":"Attributes"},{"location":"tabu_search/#methods","text":".solve() Executes the algorithm solution with the current parameters. Results will be stored to the class attributes. If the user opted to store intermediate results, these will also be stored. Parameters None Returns None","title":"Methods"},{"location":"tabu_search/#example","text":"from optiseek.metaheuristics import tabu_search from optiseek.testfunctions import booth # create an instance of the algorithm, set its parameters, and solve alg = tabu_search(booth) # create instance with booth test function alg.b_lower = [-10, -10] # define lower bounds alg.b_upper = [10, 10] # define upper bounds alg.max_iter = 100 # set iteration limit alg.sol_threshold = 0.001 # set a solution threshold alg.sigma_coeff = 0.02 # set a multiplier of bound widths for std. dev. alg.neighbor_dim_changes = 1 # only mutate 1 dimension at a time for neighbors alg.initial_guess = [2, 5] # set an initial guess of the optimum alg.tenure = 7 # set the length of the tabu list to 7 previous positions alg.n_candidates = 4 # generate four neighbors during each iteration and pick best alg.neighbor_tolerance = 0.015 # fraction of bound widths to use for tolerance # execute the algorithm alg.solve() # show the results! print(alg.best_value) print(alg.best_position) print(alg.completed_iter)","title":"Example"},{"location":"tabu_search/#references","text":"Tabu Search on Wikipedia","title":"References"},{"location":"wheelers_ridge/","text":"Wheeler's Ridge This is a 2D function with a global minimum in a deep valley. It is mostly smooth other than two ridges along each of the principal axes. These cause some algorithms to converge into local minima or diverge. In this form, the minimum is at [1, 1.5] with a value of -1. Form of the function is as follows: f(x, y) = -exp(-(x1*x2 - 1.5)^2 - (x2 - 1.5)^2) function optiseek.testfunctions. wheelers_ridge ( x1, x2 ) Parameters Parameter Description x1 : float Input value for the first dimension. x2 : float Input value for the second dimension. Example from optiseek.testfunctions import wheelers_ridge y = wheelers_ridge(1, 1.5) References List of Test Functions on Wikipedia","title":"Wheelers Ridge"},{"location":"wheelers_ridge/#wheelers-ridge","text":"This is a 2D function with a global minimum in a deep valley. It is mostly smooth other than two ridges along each of the principal axes. These cause some algorithms to converge into local minima or diverge. In this form, the minimum is at [1, 1.5] with a value of -1. Form of the function is as follows: f(x, y) = -exp(-(x1*x2 - 1.5)^2 - (x2 - 1.5)^2) function optiseek.testfunctions. wheelers_ridge ( x1, x2 )","title":"Wheeler's  Ridge"},{"location":"wheelers_ridge/#parameters","text":"Parameter Description x1 : float Input value for the first dimension. x2 : float Input value for the second dimension.","title":"Parameters"},{"location":"wheelers_ridge/#example","text":"from optiseek.testfunctions import wheelers_ridge y = wheelers_ridge(1, 1.5)","title":"Example"},{"location":"wheelers_ridge/#references","text":"List of Test Functions on Wikipedia","title":"References"}]}