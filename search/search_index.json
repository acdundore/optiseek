{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"optiseek An open source collection of single-objective optimization algorithms for multi-dimensional functions. The purpose of this library is to give users access to a variety of black-box optimization algorithms with extreme ease of use and interoperability. The parameters of each of the algorithms can be tuned by the users and there is a high level of parameter uniformity between algorithms of similar type. Installation pip install optiseek Usage optiseek provides access to numerous optimization algorithms that require minimal effort from the user. An example using the well-known particle swarm optimization algorithm can be as simple as this: from optiseek.metaheuristics import particle_swarm_optimizer from optiseek.testfunctions import booth # create an instance of the algorithm, set its parameters, and solve my_algorithm = particle_swarm_optimizer(booth) # create instance to optimize the booth function my_algorithm.b_lower = [-10, -10] # define lower bounds my_algorithm.b_upper = [10, 10] # define upper bounds # execute the algorithm my_algorithm.solve() # show the results! print(my_algorithm.best_value) print(my_algorithm.best_position) print(my_algorithm.completed_iter) This is a fairly basic example implementation without much thought put into parameter selection. Of course, the user is free to tune the parameters of the algorithm any way they would like. License optiseek was created by Alex Dundore. It is licensed under the terms of the MIT license. Credits and Dependencies optiseek is powered by numpy .","title":"Home"},{"location":"#optiseek","text":"An open source collection of single-objective optimization algorithms for multi-dimensional functions. The purpose of this library is to give users access to a variety of black-box optimization algorithms with extreme ease of use and interoperability. The parameters of each of the algorithms can be tuned by the users and there is a high level of parameter uniformity between algorithms of similar type.","title":"optiseek"},{"location":"#installation","text":"pip install optiseek","title":"Installation"},{"location":"#usage","text":"optiseek provides access to numerous optimization algorithms that require minimal effort from the user. An example using the well-known particle swarm optimization algorithm can be as simple as this: from optiseek.metaheuristics import particle_swarm_optimizer from optiseek.testfunctions import booth # create an instance of the algorithm, set its parameters, and solve my_algorithm = particle_swarm_optimizer(booth) # create instance to optimize the booth function my_algorithm.b_lower = [-10, -10] # define lower bounds my_algorithm.b_upper = [10, 10] # define upper bounds # execute the algorithm my_algorithm.solve() # show the results! print(my_algorithm.best_value) print(my_algorithm.best_position) print(my_algorithm.completed_iter) This is a fairly basic example implementation without much thought put into parameter selection. Of course, the user is free to tune the parameters of the algorithm any way they would like.","title":"Usage"},{"location":"#license","text":"optiseek was created by Alex Dundore. It is licensed under the terms of the MIT license.","title":"License"},{"location":"#credits-and-dependencies","text":"optiseek is powered by numpy .","title":"Credits and Dependencies"},{"location":"ackley2D/","text":"Ackley's Function (2-Dimensional) This is a non-convex function with many local optima around a single global minimum of zero at [0, 0]. Form of the function is as follows: f(x, y) = -20exp(-0.2sqrt(0.5(x1^2+x2^2))) - exp(0.5(cos(2\u03c0x1) + cos(2\u03c0x2))) + exp(1) + 20 function optiseek.testfunctions. ackley2D ( x1, x2 ) Parameters Parameter Description x1 : float Input value for the first dimension. x2 : float Input value for the second dimension. Example from optiseek.testfunctions import ackley2D y = ackley2D(0, 0) References List of Test Functions on Wikipedia","title":"Ackleys Function (2D)"},{"location":"ackley2D/#ackleys-function-2-dimensional","text":"This is a non-convex function with many local optima around a single global minimum of zero at [0, 0]. Form of the function is as follows: f(x, y) = -20exp(-0.2sqrt(0.5(x1^2+x2^2))) - exp(0.5(cos(2\u03c0x1) + cos(2\u03c0x2))) + exp(1) + 20 function optiseek.testfunctions. ackley2D ( x1, x2 )","title":"Ackley's Function (2-Dimensional)"},{"location":"ackley2D/#parameters","text":"Parameter Description x1 : float Input value for the first dimension. x2 : float Input value for the second dimension.","title":"Parameters"},{"location":"ackley2D/#example","text":"from optiseek.testfunctions import ackley2D y = ackley2D(0, 0)","title":"Example"},{"location":"ackley2D/#references","text":"List of Test Functions on Wikipedia","title":"References"},{"location":"basic_pattern_search/","text":"Basic Pattern Search This class represents a basic Hooke-Jeeves pattern search algorithm. This is a basic black-box optimization function that requires no knowledge of the form of the function to be optimized. The algorithm starts by selecting two sample points (one on each side of the current position, +/- the step size) for each dimension. If the best sample point is better than the current position, the algorithm sets the current position to the best sample point. Otherwise, the step size halves and the algorithm continues iterating. This method is effective and requires 2n function evaluations for each iteration, where n is the number of dimensions. It is also susceptible to getting stuck in local optima. class optiseek.metaheuristics. basic_pattern_search ( input_function, initial_guess, find_minimum=True, max_iter=100, sol_threshold=None, store_results=False, max_step_size=1.0 ) Parameters Parameter Description input_function : function Function that the algorithm will use to search for an optimum. *args will be passed to the function within the solver. initial_guess : float, list of floats, or ndarray The initial guess that the algorithm will start the search from. Can be a float if the function is one-dimensional. find_minimum : bool Indicates whether the optimimum of interest is a minimum or maximum. If true, looks for minimum. If false, looks for maximum. max_iter : int Maximum number of iterations. If reached, the algorithm terminates. sol_threshold : float If a solution is found better than this threshold, the iterations stop. None indicates that the algorithm will not consider this. store_results : bool Choose whether to save intermediate iteration results for post-processing or not. If true, results will be saved. max_step_size : float Maximum step size that the algorithm can possibly take for each iteration in each direction. Attributes Attribute Description best_position : ndarray Most optimal position found during the solution iterations. best_value : float Most optimal function value found during the solution iterations. completed_iter : int Number of iterations completed during the solution process. stored_positions : ndarray Positions for each member of the population for each iteration after the solver is finished. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, position in each dimension] stored_values : ndarray Function values for each member of the population for each iteration. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, function value] Methods .solve() Executes the algorithm solution with the current parameters. Results will be stored to the class attributes. If the user opted to store intermediate results, these will also be stored. Parameters None Returns None Example from optiseek.direct import basic_pattern_search from optiseek.testfunctions import booth # create an instance of the algorithm, set its parameters, and solve alg = basic_pattern_search(booth, [0, 0]) # create instance with booth test function and initial guess [0, 0] alg.max_iter = 100 # set iteration limit alg.sol_threshold = 0.001 # set a solution threshold alg.max_step_size = 0.5 # define maximum step size # execute the algorithm alg.solve() # show the results! print(alg.best_value) print(alg.best_position) print(alg.completed_iter) References Hooke-Jeeves Pattern Search on Wikipedia","title":"Basic Pattern Search"},{"location":"basic_pattern_search/#basic-pattern-search","text":"This class represents a basic Hooke-Jeeves pattern search algorithm. This is a basic black-box optimization function that requires no knowledge of the form of the function to be optimized. The algorithm starts by selecting two sample points (one on each side of the current position, +/- the step size) for each dimension. If the best sample point is better than the current position, the algorithm sets the current position to the best sample point. Otherwise, the step size halves and the algorithm continues iterating. This method is effective and requires 2n function evaluations for each iteration, where n is the number of dimensions. It is also susceptible to getting stuck in local optima. class optiseek.metaheuristics. basic_pattern_search ( input_function, initial_guess, find_minimum=True, max_iter=100, sol_threshold=None, store_results=False, max_step_size=1.0 )","title":"Basic Pattern Search"},{"location":"basic_pattern_search/#parameters","text":"Parameter Description input_function : function Function that the algorithm will use to search for an optimum. *args will be passed to the function within the solver. initial_guess : float, list of floats, or ndarray The initial guess that the algorithm will start the search from. Can be a float if the function is one-dimensional. find_minimum : bool Indicates whether the optimimum of interest is a minimum or maximum. If true, looks for minimum. If false, looks for maximum. max_iter : int Maximum number of iterations. If reached, the algorithm terminates. sol_threshold : float If a solution is found better than this threshold, the iterations stop. None indicates that the algorithm will not consider this. store_results : bool Choose whether to save intermediate iteration results for post-processing or not. If true, results will be saved. max_step_size : float Maximum step size that the algorithm can possibly take for each iteration in each direction.","title":"Parameters"},{"location":"basic_pattern_search/#attributes","text":"Attribute Description best_position : ndarray Most optimal position found during the solution iterations. best_value : float Most optimal function value found during the solution iterations. completed_iter : int Number of iterations completed during the solution process. stored_positions : ndarray Positions for each member of the population for each iteration after the solver is finished. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, position in each dimension] stored_values : ndarray Function values for each member of the population for each iteration. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, function value]","title":"Attributes"},{"location":"basic_pattern_search/#methods","text":".solve() Executes the algorithm solution with the current parameters. Results will be stored to the class attributes. If the user opted to store intermediate results, these will also be stored. Parameters None Returns None","title":"Methods"},{"location":"basic_pattern_search/#example","text":"from optiseek.direct import basic_pattern_search from optiseek.testfunctions import booth # create an instance of the algorithm, set its parameters, and solve alg = basic_pattern_search(booth, [0, 0]) # create instance with booth test function and initial guess [0, 0] alg.max_iter = 100 # set iteration limit alg.sol_threshold = 0.001 # set a solution threshold alg.max_step_size = 0.5 # define maximum step size # execute the algorithm alg.solve() # show the results! print(alg.best_value) print(alg.best_position) print(alg.completed_iter)","title":"Example"},{"location":"basic_pattern_search/#references","text":"Hooke-Jeeves Pattern Search on Wikipedia","title":"References"},{"location":"booth/","text":"Booth's Function This is a simple 2D quadratic function with a minimum of zero at [1, 3]. Form of the function is as follows: f(x, y) = (x + 2y - 7)^2 + (2x + y - 5)^2 function optiseek.testfunctions. booth ( x1, x2 ) Parameters Parameter Description x1 : float Input value for the first dimension. x2 : float Input value for the second dimension. Example from optiseek.testfunctions import booth y = booth(1, 3) References List of Test Functions on Wikipedia","title":"Booths Function"},{"location":"booth/#booths-function","text":"This is a simple 2D quadratic function with a minimum of zero at [1, 3]. Form of the function is as follows: f(x, y) = (x + 2y - 7)^2 + (2x + y - 5)^2 function optiseek.testfunctions. booth ( x1, x2 )","title":"Booth's Function"},{"location":"booth/#parameters","text":"Parameter Description x1 : float Input value for the first dimension. x2 : float Input value for the second dimension.","title":"Parameters"},{"location":"booth/#example","text":"from optiseek.testfunctions import booth y = booth(1, 3)","title":"Example"},{"location":"booth/#references","text":"List of Test Functions on Wikipedia","title":"References"},{"location":"cyclic_coordinate_descent/","text":"Cyclic Coordinate Descent This class represents a cyclic coordinate descent algorithm. This is a basic black-box optimization function that requires no knowledge of the form of the function to be optimized. The algorithm cycles through each of the dimensions in sequence and does an individual line search (a golden section search) within the maximum step size specified by the user. While the line search is executed in a certain dimension, the position values in all other dimensions are held constant. This is a deterministic method that is susceptible to getting stuck in local optima. In some cases, the algorithm gets stuck in a loop before it even reaches a local optimum. In these cases, changing the initial guess can rectify the issue. class optiseek.metaheuristics. cyclic_coordinate_descent ( input_function, initial_guess, find_minimum=True, max_iter=100, sol_threshold=None, store_results=False, max_step_size=1.0 ) Parameters Parameter Description input_function : function Function that the algorithm will use to search for an optimum. *args will be passed to the function within the solver. initial_guess : float, list of floats, or ndarray The initial guess that the algorithm will start the search from. Can be a float if the function is one-dimensional. find_minimum : bool Indicates whether the optimimum of interest is a minimum or maximum. If true, looks for minimum. If false, looks for maximum. max_iter : int Maximum number of iterations. If reached, the algorithm terminates. sol_threshold : float If a solution is found better than this threshold, the iterations stop. None indicates that the algorithm will not consider this. store_results : bool Choose whether to save intermediate iteration results for post-processing or not. If true, results will be saved. max_step_size : float Maximum step size that the algorithm can possibly take for each iteration in each direction. Attributes Attribute Description best_position : ndarray Most optimal position found during the solution iterations. best_value : float Most optimal function value found during the solution iterations. completed_iter : int Number of iterations completed during the solution process. stored_positions : ndarray Positions for each member of the population for each iteration after the solver is finished. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, position in each dimension] stored_values : ndarray Function values for each member of the population for each iteration. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, function value] Methods .solve() Executes the algorithm solution with the current parameters. Results will be stored to the class attributes. If the user opted to store intermediate results, these will also be stored. Parameters None Returns None Example from optiseek.direct import cyclic_coordinate_descent from optiseek.testfunctions import booth # create an instance of the algorithm, set its parameters, and solve alg = cyclic_coordinate_descent(booth, [0, 0]) # create instance with booth test function and initial guess [0, 0] alg.max_iter = 100 # set iteration limit alg.sol_threshold = 0.001 # set a solution threshold alg.max_step_size = 0.5 # define maximum step size # execute the algorithm alg.solve() # show the results! print(alg.best_value) print(alg.best_position) print(alg.completed_iter) References Coordinate Descent on Wikipedia","title":"Cyclic Coordinate Descent"},{"location":"cyclic_coordinate_descent/#cyclic-coordinate-descent","text":"This class represents a cyclic coordinate descent algorithm. This is a basic black-box optimization function that requires no knowledge of the form of the function to be optimized. The algorithm cycles through each of the dimensions in sequence and does an individual line search (a golden section search) within the maximum step size specified by the user. While the line search is executed in a certain dimension, the position values in all other dimensions are held constant. This is a deterministic method that is susceptible to getting stuck in local optima. In some cases, the algorithm gets stuck in a loop before it even reaches a local optimum. In these cases, changing the initial guess can rectify the issue. class optiseek.metaheuristics. cyclic_coordinate_descent ( input_function, initial_guess, find_minimum=True, max_iter=100, sol_threshold=None, store_results=False, max_step_size=1.0 )","title":"Cyclic Coordinate Descent"},{"location":"cyclic_coordinate_descent/#parameters","text":"Parameter Description input_function : function Function that the algorithm will use to search for an optimum. *args will be passed to the function within the solver. initial_guess : float, list of floats, or ndarray The initial guess that the algorithm will start the search from. Can be a float if the function is one-dimensional. find_minimum : bool Indicates whether the optimimum of interest is a minimum or maximum. If true, looks for minimum. If false, looks for maximum. max_iter : int Maximum number of iterations. If reached, the algorithm terminates. sol_threshold : float If a solution is found better than this threshold, the iterations stop. None indicates that the algorithm will not consider this. store_results : bool Choose whether to save intermediate iteration results for post-processing or not. If true, results will be saved. max_step_size : float Maximum step size that the algorithm can possibly take for each iteration in each direction.","title":"Parameters"},{"location":"cyclic_coordinate_descent/#attributes","text":"Attribute Description best_position : ndarray Most optimal position found during the solution iterations. best_value : float Most optimal function value found during the solution iterations. completed_iter : int Number of iterations completed during the solution process. stored_positions : ndarray Positions for each member of the population for each iteration after the solver is finished. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, position in each dimension] stored_values : ndarray Function values for each member of the population for each iteration. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, function value]","title":"Attributes"},{"location":"cyclic_coordinate_descent/#methods","text":".solve() Executes the algorithm solution with the current parameters. Results will be stored to the class attributes. If the user opted to store intermediate results, these will also be stored. Parameters None Returns None","title":"Methods"},{"location":"cyclic_coordinate_descent/#example","text":"from optiseek.direct import cyclic_coordinate_descent from optiseek.testfunctions import booth # create an instance of the algorithm, set its parameters, and solve alg = cyclic_coordinate_descent(booth, [0, 0]) # create instance with booth test function and initial guess [0, 0] alg.max_iter = 100 # set iteration limit alg.sol_threshold = 0.001 # set a solution threshold alg.max_step_size = 0.5 # define maximum step size # execute the algorithm alg.solve() # show the results! print(alg.best_value) print(alg.best_position) print(alg.completed_iter)","title":"Example"},{"location":"cyclic_coordinate_descent/#references","text":"Coordinate Descent on Wikipedia","title":"References"},{"location":"differential_evolution/","text":"Differential Evolution This class represents the differential evolution algorithm developed by Storn and Price. This is an evolutionary algorithm that utilizes vector-based genetic crossovers. It contains the typical components of a genetic algorithm (mutation, crossover, & selection) but has a special unique form of crossover that makes it widely applicable to a diverse set of problems. There are also very few parameters, making it simple to tune. class optiseek.metaheuristics. differential_evolution ( input_function, b_lower=-10, b_upper=10, find_minimum=True, max_iter=100, sol_threshold=None, max_unchanged_iter=None, store_results=False, n_agents=50, weight=0.2, p_crossover=0.5 ) Parameters Parameter Description input_function : function Function that the algorithm will use to search for an optimum. *args will be passed to the function within the solver. b_lower : float, list of floats, or ndarray Contains the lower bounds of each dimension in the search space. Can be a float if the function is one-dimensional. b_upper : float, list of floats, or ndarray Contains the upper bounds of each dimension in the search space. Can be a float if the function is one-dimensional. find_minimum : bool Indicates whether the optimimum of interest is a minimum or maximum. If true, looks for minimum. If false, looks for maximum. max_iter : int Maximum number of iterations. If reached, the algorithm terminates. sol_threshold : float If a solution is found better than this threshold, the iterations stop. None indicates that the algorithm will not consider this. max_unchanged_iter : int If the solution does not improve after this many iterations, the solver terminates. None indicates that the algorithm will not consider this. store_results : bool Choose whether to save intermediate iteration results for post-processing or not. If true, results will be saved. n_agents : int Number of agents to use in the population. weight : float Differential weight coefficient in [0, 2]. p_crossover : float Probability in [0, 1] that a gene crossover will occur for each dimension. Attributes Attribute Description best_position : ndarray Most optimal position found during the solution iterations. best_value : float Most optimal function value found during the solution iterations. completed_iter : int Number of iterations completed during the solution process. stored_positions : ndarray Positions for each member of the population for each iteration after the solver is finished. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, position in each dimension] stored_values : ndarray Function values for each member of the population for each iteration. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, function value] Methods .solve() Executes the algorithm solution with the current parameters. Results will be stored to the class attributes. If the user opted to store intermediate results, these will also be stored. Parameters None Returns None Example from optiseek.metaheuristics import differential_evolution from optiseek.testfunctions import booth # create an instance of the algorithm, set its parameters, and solve alg = differential_evolution(booth) # create instance with booth test function alg.b_lower = [-10, -10] # define lower bounds alg.b_upper = [10, 10] # define upper bounds alg.max_iter = 100 # set iteration limit alg.sol_threshold = 0.001 # set a solution threshold alg.n_agents = 20 # define population size alg.weight = 0.2 # set differential weight alg.p_crossover = 0.35 # set crossover probability # execute the algorithm alg.solve() # show the results! print(alg.best_value) print(alg.best_position) print(alg.completed_iter) References Differential Evolution on Wikipedia","title":"Differential Evolution"},{"location":"differential_evolution/#differential-evolution","text":"This class represents the differential evolution algorithm developed by Storn and Price. This is an evolutionary algorithm that utilizes vector-based genetic crossovers. It contains the typical components of a genetic algorithm (mutation, crossover, & selection) but has a special unique form of crossover that makes it widely applicable to a diverse set of problems. There are also very few parameters, making it simple to tune. class optiseek.metaheuristics. differential_evolution ( input_function, b_lower=-10, b_upper=10, find_minimum=True, max_iter=100, sol_threshold=None, max_unchanged_iter=None, store_results=False, n_agents=50, weight=0.2, p_crossover=0.5 )","title":"Differential Evolution"},{"location":"differential_evolution/#parameters","text":"Parameter Description input_function : function Function that the algorithm will use to search for an optimum. *args will be passed to the function within the solver. b_lower : float, list of floats, or ndarray Contains the lower bounds of each dimension in the search space. Can be a float if the function is one-dimensional. b_upper : float, list of floats, or ndarray Contains the upper bounds of each dimension in the search space. Can be a float if the function is one-dimensional. find_minimum : bool Indicates whether the optimimum of interest is a minimum or maximum. If true, looks for minimum. If false, looks for maximum. max_iter : int Maximum number of iterations. If reached, the algorithm terminates. sol_threshold : float If a solution is found better than this threshold, the iterations stop. None indicates that the algorithm will not consider this. max_unchanged_iter : int If the solution does not improve after this many iterations, the solver terminates. None indicates that the algorithm will not consider this. store_results : bool Choose whether to save intermediate iteration results for post-processing or not. If true, results will be saved. n_agents : int Number of agents to use in the population. weight : float Differential weight coefficient in [0, 2]. p_crossover : float Probability in [0, 1] that a gene crossover will occur for each dimension.","title":"Parameters"},{"location":"differential_evolution/#attributes","text":"Attribute Description best_position : ndarray Most optimal position found during the solution iterations. best_value : float Most optimal function value found during the solution iterations. completed_iter : int Number of iterations completed during the solution process. stored_positions : ndarray Positions for each member of the population for each iteration after the solver is finished. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, position in each dimension] stored_values : ndarray Function values for each member of the population for each iteration. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, function value]","title":"Attributes"},{"location":"differential_evolution/#methods","text":".solve() Executes the algorithm solution with the current parameters. Results will be stored to the class attributes. If the user opted to store intermediate results, these will also be stored. Parameters None Returns None","title":"Methods"},{"location":"differential_evolution/#example","text":"from optiseek.metaheuristics import differential_evolution from optiseek.testfunctions import booth # create an instance of the algorithm, set its parameters, and solve alg = differential_evolution(booth) # create instance with booth test function alg.b_lower = [-10, -10] # define lower bounds alg.b_upper = [10, 10] # define upper bounds alg.max_iter = 100 # set iteration limit alg.sol_threshold = 0.001 # set a solution threshold alg.n_agents = 20 # define population size alg.weight = 0.2 # set differential weight alg.p_crossover = 0.35 # set crossover probability # execute the algorithm alg.solve() # show the results! print(alg.best_value) print(alg.best_position) print(alg.completed_iter)","title":"Example"},{"location":"differential_evolution/#references","text":"Differential Evolution on Wikipedia","title":"References"},{"location":"enhanced_pattern_search/","text":"Enhanced Pattern Search This class represents an enhanced Hooke-Jeeves pattern search algorithm. This is a basic black-box optimization function that requires no knowledge of the form of the function to be optimized. It is very similar to the standard Hooke-Jeeves Pattern Search algorithm with a slight variation. Rather than taking two sample points in each dimension for each iteration, a single sample point is taken for each dimension in the positive direction with the magnitude of the step size. Then, a final sample point is taken in the negative direction of the step size in all dimensions at once. This results in a positive spanning set of search directions, with only n+1 sample points taken at each iteration. This can be more efficient than the basic Hooke-Jeeves method. class optiseek.metaheuristics. enhanced_pattern_search ( input_function, initial_guess, find_minimum=True, max_iter=100, sol_threshold=None, store_results=False, max_step_size=1.0 ) Parameters Parameter Description input_function : function Function that the algorithm will use to search for an optimum. *args will be passed to the function within the solver. initial_guess : float, list of floats, or ndarray The initial guess that the algorithm will start the search from. Can be a float if the function is one-dimensional. find_minimum : bool Indicates whether the optimimum of interest is a minimum or maximum. If true, looks for minimum. If false, looks for maximum. max_iter : int Maximum number of iterations. If reached, the algorithm terminates. sol_threshold : float If a solution is found better than this threshold, the iterations stop. None indicates that the algorithm will not consider this. store_results : bool Choose whether to save intermediate iteration results for post-processing or not. If true, results will be saved. max_step_size : float Maximum step size that the algorithm can possibly take for each iteration in each direction. Attributes Attribute Description best_position : ndarray Most optimal position found during the solution iterations. best_value : float Most optimal function value found during the solution iterations. completed_iter : int Number of iterations completed during the solution process. stored_positions : ndarray Positions for each member of the population for each iteration after the solver is finished. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, position in each dimension] stored_values : ndarray Function values for each member of the population for each iteration. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, function value] Methods .solve() Executes the algorithm solution with the current parameters. Results will be stored to the class attributes. If the user opted to store intermediate results, these will also be stored. Parameters None Returns None Example from optiseek.direct import enhanced_pattern_search from optiseek.testfunctions import booth # create an instance of the algorithm, set its parameters, and solve alg = enhanced_pattern_search(booth, [0, 0]) # create instance with booth test function and initial guess [0, 0] alg.max_iter = 100 # set iteration limit alg.sol_threshold = 0.001 # set a solution threshold alg.max_step_size = 0.5 # define maximum step size # execute the algorithm alg.solve() # show the results! print(alg.best_value) print(alg.best_position) print(alg.completed_iter) References Hooke-Jeeves Pattern Search on Wikipedia","title":"Enhanced Pattern Search"},{"location":"enhanced_pattern_search/#enhanced-pattern-search","text":"This class represents an enhanced Hooke-Jeeves pattern search algorithm. This is a basic black-box optimization function that requires no knowledge of the form of the function to be optimized. It is very similar to the standard Hooke-Jeeves Pattern Search algorithm with a slight variation. Rather than taking two sample points in each dimension for each iteration, a single sample point is taken for each dimension in the positive direction with the magnitude of the step size. Then, a final sample point is taken in the negative direction of the step size in all dimensions at once. This results in a positive spanning set of search directions, with only n+1 sample points taken at each iteration. This can be more efficient than the basic Hooke-Jeeves method. class optiseek.metaheuristics. enhanced_pattern_search ( input_function, initial_guess, find_minimum=True, max_iter=100, sol_threshold=None, store_results=False, max_step_size=1.0 )","title":"Enhanced Pattern Search"},{"location":"enhanced_pattern_search/#parameters","text":"Parameter Description input_function : function Function that the algorithm will use to search for an optimum. *args will be passed to the function within the solver. initial_guess : float, list of floats, or ndarray The initial guess that the algorithm will start the search from. Can be a float if the function is one-dimensional. find_minimum : bool Indicates whether the optimimum of interest is a minimum or maximum. If true, looks for minimum. If false, looks for maximum. max_iter : int Maximum number of iterations. If reached, the algorithm terminates. sol_threshold : float If a solution is found better than this threshold, the iterations stop. None indicates that the algorithm will not consider this. store_results : bool Choose whether to save intermediate iteration results for post-processing or not. If true, results will be saved. max_step_size : float Maximum step size that the algorithm can possibly take for each iteration in each direction.","title":"Parameters"},{"location":"enhanced_pattern_search/#attributes","text":"Attribute Description best_position : ndarray Most optimal position found during the solution iterations. best_value : float Most optimal function value found during the solution iterations. completed_iter : int Number of iterations completed during the solution process. stored_positions : ndarray Positions for each member of the population for each iteration after the solver is finished. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, position in each dimension] stored_values : ndarray Function values for each member of the population for each iteration. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, function value]","title":"Attributes"},{"location":"enhanced_pattern_search/#methods","text":".solve() Executes the algorithm solution with the current parameters. Results will be stored to the class attributes. If the user opted to store intermediate results, these will also be stored. Parameters None Returns None","title":"Methods"},{"location":"enhanced_pattern_search/#example","text":"from optiseek.direct import enhanced_pattern_search from optiseek.testfunctions import booth # create an instance of the algorithm, set its parameters, and solve alg = enhanced_pattern_search(booth, [0, 0]) # create instance with booth test function and initial guess [0, 0] alg.max_iter = 100 # set iteration limit alg.sol_threshold = 0.001 # set a solution threshold alg.max_step_size = 0.5 # define maximum step size # execute the algorithm alg.solve() # show the results! print(alg.best_value) print(alg.best_position) print(alg.completed_iter)","title":"Example"},{"location":"enhanced_pattern_search/#references","text":"Hooke-Jeeves Pattern Search on Wikipedia","title":"References"},{"location":"firefly_algorithm/","text":"Firefly Algorithm This class represents the firefly algorithm developed by Xin-She Yang. This algorithm is based on the flashing patterns and swarm behavior of fireflies. Fireflies are attracted to others based on their proximity in the search space and the brightness (function values) of others. Their movements also have a stochastic component. class optiseek.metaheuristics. firefly_algorithm ( input_function, b_lower=-10, b_upper=10, find_minimum=True, max_iter=100, sol_threshold=None, max_unchanged_iter=None, store_results=False, n_fireflies=50, beta=1.0, alpha=0.01, gamma=1.0 ) Parameters Parameter Description input_function : function Function that the algorithm will use to search for an optimum. *args will be passed to the function within the solver. b_lower : float, list of floats, or ndarray Contains the lower bounds of each dimension in the search space. Can be a float if the function is one-dimensional. b_upper : float, list of floats, or ndarray Contains the upper bounds of each dimension in the search space. Can be a float if the function is one-dimensional. find_minimum : bool Indicates whether the optimimum of interest is a minimum or maximum. If true, looks for minimum. If false, looks for maximum. max_iter : int Maximum number of iterations. If reached, the algorithm terminates. sol_threshold : float If a solution is found better than this threshold, the iterations stop. None indicates that the algorithm will not consider this. max_unchanged_iter : int If the solution does not improve after this many iterations, the solver terminates. None indicates that the algorithm will not consider this. store_results : bool Choose whether to save intermediate iteration results for post-processing or not. If true, results will be saved. n_fireflies : int Number of fireflies to use in the swarm population. beta : float Linear visibility coefficient in [0.1, 1.5]. Lower value indicates that the fireflies are less attracted to each other. alpha : float Coefficient in [0, 0.1] that is a portion of each dimension's bound widths to use for the random walk. gamma : float Exponential visibility coefficient in [0.01, 1]. Higher value indicates that the fireflies are less attracted to each other. Attributes Attribute Description best_position : ndarray Most optimal position found during the solution iterations. best_value : float Most optimal function value found during the solution iterations. completed_iter : int Number of iterations completed during the solution process. stored_positions : ndarray Positions for each member of the population for each iteration after the solver is finished. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, position in each dimension] stored_values : ndarray Function values for each member of the population for each iteration. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, function value] Methods .solve() Executes the algorithm solution with the current parameters. Results will be stored to the class attributes. If the user opted to store intermediate results, these will also be stored. Parameters None Returns None Example from optiseek.metaheuristics import firefly_algorithm from optiseek.testfunctions import booth # create an instance of the algorithm, set its parameters, and solve alg = firefly_algorithm(booth) # create instance with booth test function alg.b_lower = [-10, -10] # define lower bounds alg.b_upper = [10, 10] # define upper bounds alg.max_iter = 100 # set iteration limit alg.sol_threshold = 0.001 # set a solution threshold alg.n_fireflies = 20 # define population size alg.beta = 0.3 # set linear visibility coefficient alg.alpha = 0.05 # set random walk coefficient alg.gamma = 1.5 # set exponential visibility coefficient # execute the algorithm alg.solve() # show the results! print(alg.best_value) print(alg.best_position) print(alg.completed_iter) References Firefly Algorithm on Wikipedia","title":"Firefly Algorithm"},{"location":"firefly_algorithm/#firefly-algorithm","text":"This class represents the firefly algorithm developed by Xin-She Yang. This algorithm is based on the flashing patterns and swarm behavior of fireflies. Fireflies are attracted to others based on their proximity in the search space and the brightness (function values) of others. Their movements also have a stochastic component. class optiseek.metaheuristics. firefly_algorithm ( input_function, b_lower=-10, b_upper=10, find_minimum=True, max_iter=100, sol_threshold=None, max_unchanged_iter=None, store_results=False, n_fireflies=50, beta=1.0, alpha=0.01, gamma=1.0 )","title":"Firefly Algorithm"},{"location":"firefly_algorithm/#parameters","text":"Parameter Description input_function : function Function that the algorithm will use to search for an optimum. *args will be passed to the function within the solver. b_lower : float, list of floats, or ndarray Contains the lower bounds of each dimension in the search space. Can be a float if the function is one-dimensional. b_upper : float, list of floats, or ndarray Contains the upper bounds of each dimension in the search space. Can be a float if the function is one-dimensional. find_minimum : bool Indicates whether the optimimum of interest is a minimum or maximum. If true, looks for minimum. If false, looks for maximum. max_iter : int Maximum number of iterations. If reached, the algorithm terminates. sol_threshold : float If a solution is found better than this threshold, the iterations stop. None indicates that the algorithm will not consider this. max_unchanged_iter : int If the solution does not improve after this many iterations, the solver terminates. None indicates that the algorithm will not consider this. store_results : bool Choose whether to save intermediate iteration results for post-processing or not. If true, results will be saved. n_fireflies : int Number of fireflies to use in the swarm population. beta : float Linear visibility coefficient in [0.1, 1.5]. Lower value indicates that the fireflies are less attracted to each other. alpha : float Coefficient in [0, 0.1] that is a portion of each dimension's bound widths to use for the random walk. gamma : float Exponential visibility coefficient in [0.01, 1]. Higher value indicates that the fireflies are less attracted to each other.","title":"Parameters"},{"location":"firefly_algorithm/#attributes","text":"Attribute Description best_position : ndarray Most optimal position found during the solution iterations. best_value : float Most optimal function value found during the solution iterations. completed_iter : int Number of iterations completed during the solution process. stored_positions : ndarray Positions for each member of the population for each iteration after the solver is finished. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, position in each dimension] stored_values : ndarray Function values for each member of the population for each iteration. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, function value]","title":"Attributes"},{"location":"firefly_algorithm/#methods","text":".solve() Executes the algorithm solution with the current parameters. Results will be stored to the class attributes. If the user opted to store intermediate results, these will also be stored. Parameters None Returns None","title":"Methods"},{"location":"firefly_algorithm/#example","text":"from optiseek.metaheuristics import firefly_algorithm from optiseek.testfunctions import booth # create an instance of the algorithm, set its parameters, and solve alg = firefly_algorithm(booth) # create instance with booth test function alg.b_lower = [-10, -10] # define lower bounds alg.b_upper = [10, 10] # define upper bounds alg.max_iter = 100 # set iteration limit alg.sol_threshold = 0.001 # set a solution threshold alg.n_fireflies = 20 # define population size alg.beta = 0.3 # set linear visibility coefficient alg.alpha = 0.05 # set random walk coefficient alg.gamma = 1.5 # set exponential visibility coefficient # execute the algorithm alg.solve() # show the results! print(alg.best_value) print(alg.best_position) print(alg.completed_iter)","title":"Example"},{"location":"firefly_algorithm/#references","text":"Firefly Algorithm on Wikipedia","title":"References"},{"location":"flying_foxes_algorithm/","text":"Flying Foxes Algorithm This class represents the flying foxes optimization algorithm developed by Zervoudakis and Tsafarakis. This algorithm is a powerful and efficient metaheuristic that takes inspiration from the group behavior of flying foxes during a heatwave. It also contains traits of other common algorithms like genetic algorithms, which are utilized during the creation of new foxes. Foxes near the coolest spot are encouraged to explore nearby areas, preserving the breadth of the search area. If the most optimal spot currently known gets too crowded, the foxes will die off and produce new ones; this is similar to the overheating that occurs in nature when they crowd around cool areas during a heatwave. This algorithm is unique in the fact that it requires no user input for parameters; instead, a fuzzy self-tuning technique is utilized to tune the algorithm parameters for each individual fox at the beginning of every iteration. This makes the algorithm simple to deploy even by inexperienced users. It also outperforms most population-based metaheuristics in many engineering problems. class optiseek.metaheuristics. flying_foxes_algorithm ( input_function, b_lower=-10, b_upper=10, find_minimum=True, max_iter=100, sol_threshold=None, max_unchanged_iter=None, store_results=False ) Parameters Parameter Description input_function : function Function that the algorithm will use to search for an optimum. *args will be passed to the function within the solver. b_lower : float, list of floats, or ndarray Contains the lower bounds of each dimension in the search space. Can be a float if the function is one-dimensional. b_upper : float, list of floats, or ndarray Contains the upper bounds of each dimension in the search space. Can be a float if the function is one-dimensional. find_minimum : bool Indicates whether the optimimum of interest is a minimum or maximum. If true, looks for minimum. If false, looks for maximum. max_iter : int Maximum number of iterations. If reached, the algorithm terminates. sol_threshold : float If a solution is found better than this threshold, the iterations stop. None indicates that the algorithm will not consider this. max_unchanged_iter : int If the solution does not improve after this many iterations, the solver terminates. None indicates that the algorithm will not consider this. store_results : bool Choose whether to save intermediate iteration results for post-processing or not. If true, results will be saved. Attributes Attribute Description best_position : ndarray Most optimal position found during the solution iterations. best_value : float Most optimal function value found during the solution iterations. completed_iter : int Number of iterations completed during the solution process. stored_positions : ndarray Positions for each member of the population for each iteration after the solver is finished. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, position in each dimension] stored_values : ndarray Function values for each member of the population for each iteration. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, function value] Methods .solve() Executes the algorithm solution with the current parameters. Results will be stored to the class attributes. If the user opted to store intermediate results, these will also be stored. Parameters None Returns None Example from optiseek.metaheuristics import flying_foxes_algorithm from optiseek.testfunctions import booth # create an instance of the algorithm, set its parameters, and solve alg = flying_foxes_algorithm(booth) # create instance with booth test function alg.b_lower = [-10, -10] # define lower bounds alg.b_upper = [10, 10] # define upper bounds alg.max_iter = 100 # set iteration limit alg.sol_threshold = 0.001 # set a solution threshold # execute the algorithm alg.solve() # show the results! print(alg.best_value) print(alg.best_position) print(alg.completed_iter) References A global optimizer inspired from the survival strategies of flying foxes, by Konstantinos and Tsafarakis","title":"Flying Foxes Algorithm"},{"location":"flying_foxes_algorithm/#flying-foxes-algorithm","text":"This class represents the flying foxes optimization algorithm developed by Zervoudakis and Tsafarakis. This algorithm is a powerful and efficient metaheuristic that takes inspiration from the group behavior of flying foxes during a heatwave. It also contains traits of other common algorithms like genetic algorithms, which are utilized during the creation of new foxes. Foxes near the coolest spot are encouraged to explore nearby areas, preserving the breadth of the search area. If the most optimal spot currently known gets too crowded, the foxes will die off and produce new ones; this is similar to the overheating that occurs in nature when they crowd around cool areas during a heatwave. This algorithm is unique in the fact that it requires no user input for parameters; instead, a fuzzy self-tuning technique is utilized to tune the algorithm parameters for each individual fox at the beginning of every iteration. This makes the algorithm simple to deploy even by inexperienced users. It also outperforms most population-based metaheuristics in many engineering problems. class optiseek.metaheuristics. flying_foxes_algorithm ( input_function, b_lower=-10, b_upper=10, find_minimum=True, max_iter=100, sol_threshold=None, max_unchanged_iter=None, store_results=False )","title":"Flying Foxes Algorithm"},{"location":"flying_foxes_algorithm/#parameters","text":"Parameter Description input_function : function Function that the algorithm will use to search for an optimum. *args will be passed to the function within the solver. b_lower : float, list of floats, or ndarray Contains the lower bounds of each dimension in the search space. Can be a float if the function is one-dimensional. b_upper : float, list of floats, or ndarray Contains the upper bounds of each dimension in the search space. Can be a float if the function is one-dimensional. find_minimum : bool Indicates whether the optimimum of interest is a minimum or maximum. If true, looks for minimum. If false, looks for maximum. max_iter : int Maximum number of iterations. If reached, the algorithm terminates. sol_threshold : float If a solution is found better than this threshold, the iterations stop. None indicates that the algorithm will not consider this. max_unchanged_iter : int If the solution does not improve after this many iterations, the solver terminates. None indicates that the algorithm will not consider this. store_results : bool Choose whether to save intermediate iteration results for post-processing or not. If true, results will be saved.","title":"Parameters"},{"location":"flying_foxes_algorithm/#attributes","text":"Attribute Description best_position : ndarray Most optimal position found during the solution iterations. best_value : float Most optimal function value found during the solution iterations. completed_iter : int Number of iterations completed during the solution process. stored_positions : ndarray Positions for each member of the population for each iteration after the solver is finished. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, position in each dimension] stored_values : ndarray Function values for each member of the population for each iteration. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, function value]","title":"Attributes"},{"location":"flying_foxes_algorithm/#methods","text":".solve() Executes the algorithm solution with the current parameters. Results will be stored to the class attributes. If the user opted to store intermediate results, these will also be stored. Parameters None Returns None","title":"Methods"},{"location":"flying_foxes_algorithm/#example","text":"from optiseek.metaheuristics import flying_foxes_algorithm from optiseek.testfunctions import booth # create an instance of the algorithm, set its parameters, and solve alg = flying_foxes_algorithm(booth) # create instance with booth test function alg.b_lower = [-10, -10] # define lower bounds alg.b_upper = [10, 10] # define upper bounds alg.max_iter = 100 # set iteration limit alg.sol_threshold = 0.001 # set a solution threshold # execute the algorithm alg.solve() # show the results! print(alg.best_value) print(alg.best_position) print(alg.completed_iter)","title":"Example"},{"location":"flying_foxes_algorithm/#references","text":"A global optimizer inspired from the survival strategies of flying foxes, by Konstantinos and Tsafarakis","title":"References"},{"location":"mayfly_algorithm/","text":"Mayfly Algorithm This class represents the mayfly algorithm developed by Zervoudakis and Tsafarakis. This algorithm takes components from swarm-based algorithms as well as genetic algorithms and combines them into a powerful hybrid algorithm based on the mating behavior of mayflies. An initial population is split into males and females, each moving in different ways. The males exhibit swarm behavior to gather towards the best male (at the best function value), similar to particle swarm optimization. The females are attracted to a matched male if the male has a better function value. In each iteration, there is a genetic crossover between the males and females and selection of the best in the population takes place. Stochastic components are introduced into the movements to avoid local optima. This is a very powerful algorithm, but requires many parameters. class optiseek.metaheuristics. mayfly_algorithm ( input_function, b_lower=-10, b_upper=10, find_minimum=True, max_iter=100, sol_threshold=None, max_unchanged_iter=None, store_results=False, n_mayflies=50, beta=0.7, gravity=0.6, alpha_cog=0.5, alpha_soc=1.5, alpha_attract=1.5, nuptial_coeff=0.05 ) Parameters Parameter Description input_function : function Function that the algorithm will use to search for an optimum. *args will be passed to the function within the solver. b_lower : float, list of floats, or ndarray Contains the lower bounds of each dimension in the search space. Can be a float if the function is one-dimensional. b_upper : float, list of floats, or ndarray Contains the upper bounds of each dimension in the search space. Can be a float if the function is one-dimensional. find_minimum : bool Indicates whether the optimimum of interest is a minimum or maximum. If true, looks for minimum. If false, looks for maximum. max_iter : int Maximum number of iterations. If reached, the algorithm terminates. sol_threshold : float If a solution is found better than this threshold, the iterations stop. None indicates that the algorithm will not consider this. max_unchanged_iter : int If the solution does not improve after this many iterations, the solver terminates. None indicates that the algorithm will not consider this. store_results : bool Choose whether to save intermediate iteration results for post-processing or not. If true, results will be saved. n_mayflies : int Number of mayflies to use in the population. beta : float Exponential visibility coefficient in [0.1, 1]. Higher value means that mayflies are less drawn towards others. gravity : float Gravity coefficient in [0.1, 1]. Lower value means that the mayflies have less momentum. alpha_cog : float Cognitive coefficient in [0, 2]. Indicates how attracted the male mayflies are to their individually best known position. alpha_soc : float Social coefficient in [0, 2]. Indicates how attracted the male mayflies are to the male swarm's best known position. alpha_attract : float Attraction coefficient in [0, 2]. Indicates how attracted the females are to their matched male counterpart. nuptial_coeff : float Nuptial coefficient in [0, 0.4], a multiplier on bound widths for each dimension used for the male and female random walks. Attributes Attribute Description best_position : ndarray Most optimal position found during the solution iterations. best_value : float Most optimal function value found during the solution iterations. completed_iter : int Number of iterations completed during the solution process. stored_positions : ndarray Positions for each member of the population for each iteration after the solver is finished. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, position in each dimension] stored_values : ndarray Function values for each member of the population for each iteration. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, function value] Methods .solve() Executes the algorithm solution with the current parameters. Results will be stored to the class attributes. If the user opted to store intermediate results, these will also be stored. Parameters None Returns None Example from optiseek.metaheuristics import mayfly_algorithm from optiseek.testfunctions import booth # create an instance of the algorithm, set its parameters, and solve alg = mayfly_algorithm(booth) # create instance with booth test function alg.b_lower = [-10, -10] # define lower bounds alg.b_upper = [10, 10] # define upper bounds alg.max_iter = 100 # set iteration limit alg.sol_threshold = 0.001 # set a solution threshold alg.n_mayflies = 20 # set mayfly population alg.beta = 0.5 # set visibility coefficient alg.gravity = 0.4 # set gravity coefficient alg.alpha_cog = 0.5 # set male cognitive coefficient alg.alpha_soc = 1.5 # set male social coefficient alg.alpha_attract = 1.0 # set female attraction coefficient alg.nuptial_coeff = 0.02 # set random walk coefficient # execute the algorithm alg.solve() # show the results! print(alg.best_value) print(alg.best_position) print(alg.completed_iter) References A mayfly optimization algorithm, by Konstantinos and Tsafarakis","title":"Mayfly Algorithm"},{"location":"mayfly_algorithm/#mayfly-algorithm","text":"This class represents the mayfly algorithm developed by Zervoudakis and Tsafarakis. This algorithm takes components from swarm-based algorithms as well as genetic algorithms and combines them into a powerful hybrid algorithm based on the mating behavior of mayflies. An initial population is split into males and females, each moving in different ways. The males exhibit swarm behavior to gather towards the best male (at the best function value), similar to particle swarm optimization. The females are attracted to a matched male if the male has a better function value. In each iteration, there is a genetic crossover between the males and females and selection of the best in the population takes place. Stochastic components are introduced into the movements to avoid local optima. This is a very powerful algorithm, but requires many parameters. class optiseek.metaheuristics. mayfly_algorithm ( input_function, b_lower=-10, b_upper=10, find_minimum=True, max_iter=100, sol_threshold=None, max_unchanged_iter=None, store_results=False, n_mayflies=50, beta=0.7, gravity=0.6, alpha_cog=0.5, alpha_soc=1.5, alpha_attract=1.5, nuptial_coeff=0.05 )","title":"Mayfly Algorithm"},{"location":"mayfly_algorithm/#parameters","text":"Parameter Description input_function : function Function that the algorithm will use to search for an optimum. *args will be passed to the function within the solver. b_lower : float, list of floats, or ndarray Contains the lower bounds of each dimension in the search space. Can be a float if the function is one-dimensional. b_upper : float, list of floats, or ndarray Contains the upper bounds of each dimension in the search space. Can be a float if the function is one-dimensional. find_minimum : bool Indicates whether the optimimum of interest is a minimum or maximum. If true, looks for minimum. If false, looks for maximum. max_iter : int Maximum number of iterations. If reached, the algorithm terminates. sol_threshold : float If a solution is found better than this threshold, the iterations stop. None indicates that the algorithm will not consider this. max_unchanged_iter : int If the solution does not improve after this many iterations, the solver terminates. None indicates that the algorithm will not consider this. store_results : bool Choose whether to save intermediate iteration results for post-processing or not. If true, results will be saved. n_mayflies : int Number of mayflies to use in the population. beta : float Exponential visibility coefficient in [0.1, 1]. Higher value means that mayflies are less drawn towards others. gravity : float Gravity coefficient in [0.1, 1]. Lower value means that the mayflies have less momentum. alpha_cog : float Cognitive coefficient in [0, 2]. Indicates how attracted the male mayflies are to their individually best known position. alpha_soc : float Social coefficient in [0, 2]. Indicates how attracted the male mayflies are to the male swarm's best known position. alpha_attract : float Attraction coefficient in [0, 2]. Indicates how attracted the females are to their matched male counterpart. nuptial_coeff : float Nuptial coefficient in [0, 0.4], a multiplier on bound widths for each dimension used for the male and female random walks.","title":"Parameters"},{"location":"mayfly_algorithm/#attributes","text":"Attribute Description best_position : ndarray Most optimal position found during the solution iterations. best_value : float Most optimal function value found during the solution iterations. completed_iter : int Number of iterations completed during the solution process. stored_positions : ndarray Positions for each member of the population for each iteration after the solver is finished. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, position in each dimension] stored_values : ndarray Function values for each member of the population for each iteration. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, function value]","title":"Attributes"},{"location":"mayfly_algorithm/#methods","text":".solve() Executes the algorithm solution with the current parameters. Results will be stored to the class attributes. If the user opted to store intermediate results, these will also be stored. Parameters None Returns None","title":"Methods"},{"location":"mayfly_algorithm/#example","text":"from optiseek.metaheuristics import mayfly_algorithm from optiseek.testfunctions import booth # create an instance of the algorithm, set its parameters, and solve alg = mayfly_algorithm(booth) # create instance with booth test function alg.b_lower = [-10, -10] # define lower bounds alg.b_upper = [10, 10] # define upper bounds alg.max_iter = 100 # set iteration limit alg.sol_threshold = 0.001 # set a solution threshold alg.n_mayflies = 20 # set mayfly population alg.beta = 0.5 # set visibility coefficient alg.gravity = 0.4 # set gravity coefficient alg.alpha_cog = 0.5 # set male cognitive coefficient alg.alpha_soc = 1.5 # set male social coefficient alg.alpha_attract = 1.0 # set female attraction coefficient alg.nuptial_coeff = 0.02 # set random walk coefficient # execute the algorithm alg.solve() # show the results! print(alg.best_value) print(alg.best_position) print(alg.completed_iter)","title":"Example"},{"location":"mayfly_algorithm/#references","text":"A mayfly optimization algorithm, by Konstantinos and Tsafarakis","title":"References"},{"location":"parameter_grid_search/","text":"Parameter Grid Search This class is a utility to help the user tune their optimization algorithms. This is accomplished by creating a grid of parameter permutations based on user input and running a selected algorithm with those parameters to find the best set. The final results of every permutation of parameters are saved for post-processing. class optiseek.modelhelpers. parameter_grid_search ( algorithm, input_function, param_options, show_progress=False ) Parameters Parameter Description algorithm : class Class of the algorithm that you would like to use. input_function : function Function object for the algorithm to optimize. param_options : dict Dictionary containing the grid of parameters to be explored with the parameter names (strings) as keys and a list of parameter values as values. All permutations of parameters in this dict will be tested. For inputs that would normally be in a list (like the search bounds on a 2+ dimensional function, for example), place that list inside another list. For any parameters not specified, the default will be used. See the example for more details. show_progress : bool Boolean to indicate whether the grid search will print progress to the console as the solve continues. The number of permutations increases exponentially with respect to parameter inputs, so for high numbers of parameter inputs, this can be useful to see how much longer the solver has left. Attributes Attribute Description param_permutations : list of dicts A list of dictionaries that represent the parameter permutations used for each iteration of the grid search. In the dicts, the parameter name as a string is the key and the parameter value is the value. permutation_positions : list of ndarrays A list containing the optimal positions found for each permutation of parameters. This corresponds with the permutations in the param_permutations attribute. permutation_values : list of floats A list containing the optimal values found for each permutation of parameters. This corresponds with the permutations in the param_permutations attribute. best_parameters : dict A dictionary containing the best performing set of parameters. The parameter names as strings are stored as keys and the corresponding values are stored as values. best_position : list or ndarray The most optimal position that was found using the best performing parameters. best_value : float The most optimal function value that was found using the best performing parameters. Methods .solve() Executes the parameter grid search process and stores the results in the class attributes. Parameters None Returns None Example from optiseek.modelhelpers import parameter_grid_search from optiseek.metaheuristics import simulated_annealing from optiseek.testfunctions import ackley2D # set up the param_options dictionary param_options = { \"initial_guess\": [[8, 4]], \"b_lower\": [[-10, -10]], \"b_upper\": [[10, 10]], \"sigma_coeff\": [0.05, 0.1, 0.2, 0.3], \"max_iter\": [50, 100, 500], \"start_temperature\": [20, 10, 5, 2], \"alpha\": [0.85, 0.93, 0.99], \"neighbor_dim_changes\": [1, 2] } # create the an instance of the grid search class and execute the solve method pgs = parameter_grid_search(simulated_annealing, ackley2D, param_options, show_progress=False) pgs.solve() # show all permutations and their associated optimal positions and values found for grid in range(len(pgs.param_permutations)): print(pgs.param_permutations[i]) print(pgs.permutation_positions) print(pgs.permutation_values) # print the best parameter permutation found and the associated optimal position and value print(pgs.best_parameters) print(pgs.best_position) print(pgs.best_value)","title":"Parameter Grid Search"},{"location":"parameter_grid_search/#parameter-grid-search","text":"This class is a utility to help the user tune their optimization algorithms. This is accomplished by creating a grid of parameter permutations based on user input and running a selected algorithm with those parameters to find the best set. The final results of every permutation of parameters are saved for post-processing. class optiseek.modelhelpers. parameter_grid_search ( algorithm, input_function, param_options, show_progress=False )","title":"Parameter Grid Search"},{"location":"parameter_grid_search/#parameters","text":"Parameter Description algorithm : class Class of the algorithm that you would like to use. input_function : function Function object for the algorithm to optimize. param_options : dict Dictionary containing the grid of parameters to be explored with the parameter names (strings) as keys and a list of parameter values as values. All permutations of parameters in this dict will be tested. For inputs that would normally be in a list (like the search bounds on a 2+ dimensional function, for example), place that list inside another list. For any parameters not specified, the default will be used. See the example for more details. show_progress : bool Boolean to indicate whether the grid search will print progress to the console as the solve continues. The number of permutations increases exponentially with respect to parameter inputs, so for high numbers of parameter inputs, this can be useful to see how much longer the solver has left.","title":"Parameters"},{"location":"parameter_grid_search/#attributes","text":"Attribute Description param_permutations : list of dicts A list of dictionaries that represent the parameter permutations used for each iteration of the grid search. In the dicts, the parameter name as a string is the key and the parameter value is the value. permutation_positions : list of ndarrays A list containing the optimal positions found for each permutation of parameters. This corresponds with the permutations in the param_permutations attribute. permutation_values : list of floats A list containing the optimal values found for each permutation of parameters. This corresponds with the permutations in the param_permutations attribute. best_parameters : dict A dictionary containing the best performing set of parameters. The parameter names as strings are stored as keys and the corresponding values are stored as values. best_position : list or ndarray The most optimal position that was found using the best performing parameters. best_value : float The most optimal function value that was found using the best performing parameters.","title":"Attributes"},{"location":"parameter_grid_search/#methods","text":".solve() Executes the parameter grid search process and stores the results in the class attributes. Parameters None Returns None","title":"Methods"},{"location":"parameter_grid_search/#example","text":"from optiseek.modelhelpers import parameter_grid_search from optiseek.metaheuristics import simulated_annealing from optiseek.testfunctions import ackley2D # set up the param_options dictionary param_options = { \"initial_guess\": [[8, 4]], \"b_lower\": [[-10, -10]], \"b_upper\": [[10, 10]], \"sigma_coeff\": [0.05, 0.1, 0.2, 0.3], \"max_iter\": [50, 100, 500], \"start_temperature\": [20, 10, 5, 2], \"alpha\": [0.85, 0.93, 0.99], \"neighbor_dim_changes\": [1, 2] } # create the an instance of the grid search class and execute the solve method pgs = parameter_grid_search(simulated_annealing, ackley2D, param_options, show_progress=False) pgs.solve() # show all permutations and their associated optimal positions and values found for grid in range(len(pgs.param_permutations)): print(pgs.param_permutations[i]) print(pgs.permutation_positions) print(pgs.permutation_values) # print the best parameter permutation found and the associated optimal position and value print(pgs.best_parameters) print(pgs.best_position) print(pgs.best_value)","title":"Example"},{"location":"particle_swarm_optimization/","text":"Particle Swarm Optimizer This class represents a standard particle swarm optimization algorithm, originally developed by Kennedy and Eberhart. This algorithm is based on swarm behavior commonly observed in nature. A population of particles is introduced to traverse the search space. Their movement is influenced by their own previous positions, the best known position of the swarm, and some stochastic velocity. class optiseek.metaheuristics. particle_swarm_optimizer ( input_function, b_lower=-10, b_upper=10, find_minimum=True, max_iter=100, sol_threshold=None, max_unchanged_iter=None, store_results=False, n_particles=50, weight=0.25, phi_p=1, phi_g=2, zero_velocity=True ) Parameters Parameter Description input_function : function Function that the algorithm will use to search for an optimum. *args will be passed to the function within the solver. b_lower : float, list of floats, or ndarray Contains the lower bounds of each dimension in the search space. Can be a float if the function is one-dimensional. b_upper : float, list of floats, or ndarray Contains the upper bounds of each dimension in the search space. Can be a float if the function is one-dimensional. find_minimum : bool Indicates whether the optimimum of interest is a minimum or maximum. If true, looks for minimum. If false, looks for maximum. max_iter : int Maximum number of iterations. If reached, the algorithm terminates. sol_threshold : float If a solution is found better than this threshold, the iterations stop. None indicates that the algorithm will not consider this. max_unchanged_iter : int If the solution does not improve after this many iterations, the solver terminates. None indicates that the algorithm will not consider this. store_results : bool Choose whether to save intermediate iteration results for post-processing or not. If true, results will be saved. n_particles : int Number of particles to use in the particle swarm population. weight : float Weight coefficient in [0, 1]. Lower weight gives the particles less momentum. phi_p : float Cognitive coefficient in [0, 3]. Higher value indicates that the particles are drawn more towards their own best known position. phi_g : float Social coefficient in [0, 3]. Higher value indicates that the particles are drawn more towards the swarm's collectively best known position. zero_velocity : bool Choose whether the particles start off with zero velocity or a random initial velocity. Initial velocities can sometimes cause divergence. Attributes Attribute Description best_position : ndarray Most optimal position found during the solution iterations. best_value : float Most optimal function value found during the solution iterations. completed_iter : int Number of iterations completed during the solution process. stored_positions : ndarray Positions for each member of the population for each iteration after the solver is finished. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, position in each dimension] stored_values : ndarray Function values for each member of the population for each iteration. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, function value] Methods .solve() Executes the algorithm solution with the current parameters. Results will be stored to the class attributes. If the user opted to store intermediate results, these will also be stored. Parameters None Returns None Example from optiseek.metaheuristics import particle_swarm_optimizer from optiseek.testfunctions import booth # create an instance of the algorithm, set its parameters, and solve alg = particle_swarm_optimizer(booth) # create instance with booth test function alg.b_lower = [-10, -10] # define lower bounds alg.b_upper = [10, 10] # define upper bounds alg.max_iter = 100 # set iteration limit alg.sol_threshold = 0.001 # set a solution threshold alg.n_particles = 20 # define population size alg.weight = 0.3 # set weight alg.phi_p = 0.5 # set cognitive coefficient alg.phi_g = 1.5 # set social coefficient # execute the algorithm alg.solve() # show the results! print(alg.best_value) print(alg.best_position) print(alg.completed_iter) References Particle Swarm Optimization on Wikipedia","title":"Particle Swarm Optimizer"},{"location":"particle_swarm_optimization/#particle-swarm-optimizer","text":"This class represents a standard particle swarm optimization algorithm, originally developed by Kennedy and Eberhart. This algorithm is based on swarm behavior commonly observed in nature. A population of particles is introduced to traverse the search space. Their movement is influenced by their own previous positions, the best known position of the swarm, and some stochastic velocity. class optiseek.metaheuristics. particle_swarm_optimizer ( input_function, b_lower=-10, b_upper=10, find_minimum=True, max_iter=100, sol_threshold=None, max_unchanged_iter=None, store_results=False, n_particles=50, weight=0.25, phi_p=1, phi_g=2, zero_velocity=True )","title":"Particle Swarm Optimizer"},{"location":"particle_swarm_optimization/#parameters","text":"Parameter Description input_function : function Function that the algorithm will use to search for an optimum. *args will be passed to the function within the solver. b_lower : float, list of floats, or ndarray Contains the lower bounds of each dimension in the search space. Can be a float if the function is one-dimensional. b_upper : float, list of floats, or ndarray Contains the upper bounds of each dimension in the search space. Can be a float if the function is one-dimensional. find_minimum : bool Indicates whether the optimimum of interest is a minimum or maximum. If true, looks for minimum. If false, looks for maximum. max_iter : int Maximum number of iterations. If reached, the algorithm terminates. sol_threshold : float If a solution is found better than this threshold, the iterations stop. None indicates that the algorithm will not consider this. max_unchanged_iter : int If the solution does not improve after this many iterations, the solver terminates. None indicates that the algorithm will not consider this. store_results : bool Choose whether to save intermediate iteration results for post-processing or not. If true, results will be saved. n_particles : int Number of particles to use in the particle swarm population. weight : float Weight coefficient in [0, 1]. Lower weight gives the particles less momentum. phi_p : float Cognitive coefficient in [0, 3]. Higher value indicates that the particles are drawn more towards their own best known position. phi_g : float Social coefficient in [0, 3]. Higher value indicates that the particles are drawn more towards the swarm's collectively best known position. zero_velocity : bool Choose whether the particles start off with zero velocity or a random initial velocity. Initial velocities can sometimes cause divergence.","title":"Parameters"},{"location":"particle_swarm_optimization/#attributes","text":"Attribute Description best_position : ndarray Most optimal position found during the solution iterations. best_value : float Most optimal function value found during the solution iterations. completed_iter : int Number of iterations completed during the solution process. stored_positions : ndarray Positions for each member of the population for each iteration after the solver is finished. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, position in each dimension] stored_values : ndarray Function values for each member of the population for each iteration. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, function value]","title":"Attributes"},{"location":"particle_swarm_optimization/#methods","text":".solve() Executes the algorithm solution with the current parameters. Results will be stored to the class attributes. If the user opted to store intermediate results, these will also be stored. Parameters None Returns None","title":"Methods"},{"location":"particle_swarm_optimization/#example","text":"from optiseek.metaheuristics import particle_swarm_optimizer from optiseek.testfunctions import booth # create an instance of the algorithm, set its parameters, and solve alg = particle_swarm_optimizer(booth) # create instance with booth test function alg.b_lower = [-10, -10] # define lower bounds alg.b_upper = [10, 10] # define upper bounds alg.max_iter = 100 # set iteration limit alg.sol_threshold = 0.001 # set a solution threshold alg.n_particles = 20 # define population size alg.weight = 0.3 # set weight alg.phi_p = 0.5 # set cognitive coefficient alg.phi_g = 1.5 # set social coefficient # execute the algorithm alg.solve() # show the results! print(alg.best_value) print(alg.best_position) print(alg.completed_iter)","title":"Example"},{"location":"particle_swarm_optimization/#references","text":"Particle Swarm Optimization on Wikipedia","title":"References"},{"location":"penalty_constraints/","text":"Penalty Constraints This function allows the user to apply constraints to an objective function as penalties and will return a new penalized function that can be optimized. Both counted penalties and quadratic penalties can be enforced with this function. Shown below is a representation of a 1D function f(x) and how it is penalized by both counted and quadratic constraints. In this case, the constraints are that the function input x cannot be less than A or greater than B . With either of these constraint types, as soon as a constraint is violated, the function value y has a penalty added to it. For the quadratic penalty, this penalty increases quadratically by the magnitude that the constraint is violated by. For the counted penalty, there is a fixed amount added to the function value for every penalty that is broken, regardless of the magnitude it is violated by. It should be noted that the counted penalty creates a sharp discontinuity in the function, while the quadratic penalty is smooth. Typically, use of the quadratic penalty will result in better convergence of the optimization algorithm; however, there are situations where the function value is optimal at the location where a constraint is applied (like at A in the graphic above). In this case, the quadratic penalty multiplier will need to approach infinity in order for the constraint to be properly enforced. In situations like this, it may be beneficial to use a moderate quadratic penalty with a small counted penalty. Applying penalty constraints does not guarantee that the optimization algorithm will converge at a point within the constraints. Oftentimes, the penalty multipliers must be tuned by the user to find out what works best. Parameters of the optiseek penalty_constraints function allow the user to control the magnitude of each of these penalties. Combinations of both of these penalty types can be applied at once with a single penalty_constraints function call. A value of zero will result in no penalty of that type being applied to the returned penalized function. A higher value increases the step height for count penalties and increases the slope for quadratic penalties. This tool will work for any function, whether the user would like to find the minimum or maximum. function optiseek.modelhelpers. penalty_constraints ( input_function, constraint_dict, find_minimum=True, p_quadratic=1, p_count=0 ) Parameters Parameter Description input_function : function Function object for the algorithm to optimize. constraint_dict : dict A dictionary that contains any number of constraint equations to be applied to the input function. The dictionary is structured like {constraint function: constraint type} where the constraints are compared to zero with a mathematical operator: g1(x) = 0, g2(x) < 0, etc. The constraint function must share the same arguments in the same order as the objective function. The constraint type must be one of the following strings: \"<\", \"<=\", \">\", \">=\", \"=\". See the example for more information. find_minimum : bool Indicates whether the optimimum of interest is a minimum or maximum. If true, looks for minimum. If false, looks for maximum. p_quadratic : float Penalty multiplier for the quadratic penalty in [0, inf]. A value of zero will result in no quadratic penalty to the objective function. A nonzero value smoothly penalizes the function according to the magnitude that the constraint is broken. Default value is 1. p_count : float Penalty multiplier for the count penalty in [0, inf]. A value of zero will result in no count penalty to the objective function. A nonzero value creates a sharp discontinuity where the constraint is broken. Default value is 0. Returns Returns Description penalized_function : function A function representing the input objective function with the constraints applied as penalties to the function value. Examples Example 1: We can demonstrate these constraints on a problem with several complex constraint equations to see it in action! Shown below is an engineering optimization problem taken from an article by Xin-She Yang (see references) based on the properties of a spring. The function, its constraints, and the variable bounds are given. Note that in this example, we choose to use a quadratic penalty multiplier of 2 and no counted penalty at all. Also note that the constraint functions must share the same arguments in the same order as the input function. from optiseek.modelhelpers import penalty_constraints from optiseek.metaheuristics import flying_foxes_algorithm # create the function definition def spring_problem(x1, x2, x3): return x1 ** 2 * x2 * (2 + x3) # create the constraints as functions def g1(x1, x2, x3): return 1 - (x2 ** 3 * x3) / (71785 * x1 ** 4) def g2(x1, x2, x3): return (4 * x2**2 - x1 * x2) / (12566 * (x1**3 * x2 - x1**4)) + (1 / (5108 * x1**2)) - 1 def g3(x1, x2, x3): return 1 - (140.45 * x1) / (x2**3 * x3) def g4(x1, x2, x3): return (x1 + x2) / 1.5 - 1 # create the constraint dictionary to define the constraint type spring_constraint_dict = {g1: \"<=\", g2: \"<=\", g3: \"<=\", g4: \"<=\"} # create a constrained version of the original function to be optimized spring_problem_constrained = penalty_constraints(spring_problem, spring_constraint_dict, find_minimum=True, p_count=0, p_quadratic=2) # instantiate an optimization algorithm with the constrained function alg = flying_foxes_algorithm(spring_problem_constrained) alg.b_lower = [0.05, 0.25, 2.0] alg.b_upper = [2.0, 1.3, 15.0] alg.max_iter = 200 # solve and show your results! alg.solve() print(alg.best_value) print(alg.best_position) print(alg.completed_iter) With this code, it can be found that the best solution is approximately: f(0.05269, 0.3834, 9.7789) = 0.012579 Example 2: In this generic problem, the constraints are a bit more difficult. The constraint_dict parameter for the penalty constraints function requires the constraints to be compared to zero. We must re-arrange the constraint equations to ensure this. Minimize: f(x, y, z) = -2x - 3y - 4z Subject to: 3x + 2y + z \u2264 10 2x + 5y + 3z \u2264 15 x, y, z \u2265 0 Re-arranged constraints: 3x + 2y + z - 10 \u2264 0 2x + 5y + 3z - 15 \u2264 0 x, y, z \u2265 0 from optiseek.modelhelpers import penalty_constraints from optiseek.metaheuristics import flying_foxes_algorithm # create the function definition def example_function(x, y, z): return -2 * x - 3 * y - 4 * z # create the constraints as functions def g1(x, y, z): return 3 * x + 2 * y + z - 10 def g2(x, y, z): return 2 * x + 5 * y + 3 * z - 15 def g3(x, y, z): return min(x, y, z) # create the constraint dictionary to define the constraint type constraint_dict = {g1: \"<=\", g2: \"<=\", g3: \">=\"} # create a constrained version of the original function to be optimized example_function_constrained = penalty_constraints(example_function, constraint_dict, p_quadratic=20) # instantiate an optimization algorithm with the constrained function alg = flying_foxes_algorithm(example_function_constrained) alg.b_lower = [0, 0, 0] alg.b_upper = [100, 100, 100] alg.max_iter = 200 # solve and show your results! alg.solve() print(alg.best_value) print(alg.best_position) print(alg.completed_iter) With this code, it can be found that the best solution is: f(0, 0, 5) = 20 References Algorithms for Optimization by Kochenderfer and Wheeler, Chapter 10.7 Spring Example Problem by Xin-She Yang","title":"Penalty Constraints"},{"location":"penalty_constraints/#penalty-constraints","text":"This function allows the user to apply constraints to an objective function as penalties and will return a new penalized function that can be optimized. Both counted penalties and quadratic penalties can be enforced with this function. Shown below is a representation of a 1D function f(x) and how it is penalized by both counted and quadratic constraints. In this case, the constraints are that the function input x cannot be less than A or greater than B . With either of these constraint types, as soon as a constraint is violated, the function value y has a penalty added to it. For the quadratic penalty, this penalty increases quadratically by the magnitude that the constraint is violated by. For the counted penalty, there is a fixed amount added to the function value for every penalty that is broken, regardless of the magnitude it is violated by. It should be noted that the counted penalty creates a sharp discontinuity in the function, while the quadratic penalty is smooth. Typically, use of the quadratic penalty will result in better convergence of the optimization algorithm; however, there are situations where the function value is optimal at the location where a constraint is applied (like at A in the graphic above). In this case, the quadratic penalty multiplier will need to approach infinity in order for the constraint to be properly enforced. In situations like this, it may be beneficial to use a moderate quadratic penalty with a small counted penalty. Applying penalty constraints does not guarantee that the optimization algorithm will converge at a point within the constraints. Oftentimes, the penalty multipliers must be tuned by the user to find out what works best. Parameters of the optiseek penalty_constraints function allow the user to control the magnitude of each of these penalties. Combinations of both of these penalty types can be applied at once with a single penalty_constraints function call. A value of zero will result in no penalty of that type being applied to the returned penalized function. A higher value increases the step height for count penalties and increases the slope for quadratic penalties. This tool will work for any function, whether the user would like to find the minimum or maximum. function optiseek.modelhelpers. penalty_constraints ( input_function, constraint_dict, find_minimum=True, p_quadratic=1, p_count=0 )","title":"Penalty Constraints"},{"location":"penalty_constraints/#parameters","text":"Parameter Description input_function : function Function object for the algorithm to optimize. constraint_dict : dict A dictionary that contains any number of constraint equations to be applied to the input function. The dictionary is structured like {constraint function: constraint type} where the constraints are compared to zero with a mathematical operator: g1(x) = 0, g2(x) < 0, etc. The constraint function must share the same arguments in the same order as the objective function. The constraint type must be one of the following strings: \"<\", \"<=\", \">\", \">=\", \"=\". See the example for more information. find_minimum : bool Indicates whether the optimimum of interest is a minimum or maximum. If true, looks for minimum. If false, looks for maximum. p_quadratic : float Penalty multiplier for the quadratic penalty in [0, inf]. A value of zero will result in no quadratic penalty to the objective function. A nonzero value smoothly penalizes the function according to the magnitude that the constraint is broken. Default value is 1. p_count : float Penalty multiplier for the count penalty in [0, inf]. A value of zero will result in no count penalty to the objective function. A nonzero value creates a sharp discontinuity where the constraint is broken. Default value is 0.","title":"Parameters"},{"location":"penalty_constraints/#returns","text":"Returns Description penalized_function : function A function representing the input objective function with the constraints applied as penalties to the function value.","title":"Returns"},{"location":"penalty_constraints/#examples","text":"","title":"Examples"},{"location":"penalty_constraints/#example-1","text":"We can demonstrate these constraints on a problem with several complex constraint equations to see it in action! Shown below is an engineering optimization problem taken from an article by Xin-She Yang (see references) based on the properties of a spring. The function, its constraints, and the variable bounds are given. Note that in this example, we choose to use a quadratic penalty multiplier of 2 and no counted penalty at all. Also note that the constraint functions must share the same arguments in the same order as the input function. from optiseek.modelhelpers import penalty_constraints from optiseek.metaheuristics import flying_foxes_algorithm # create the function definition def spring_problem(x1, x2, x3): return x1 ** 2 * x2 * (2 + x3) # create the constraints as functions def g1(x1, x2, x3): return 1 - (x2 ** 3 * x3) / (71785 * x1 ** 4) def g2(x1, x2, x3): return (4 * x2**2 - x1 * x2) / (12566 * (x1**3 * x2 - x1**4)) + (1 / (5108 * x1**2)) - 1 def g3(x1, x2, x3): return 1 - (140.45 * x1) / (x2**3 * x3) def g4(x1, x2, x3): return (x1 + x2) / 1.5 - 1 # create the constraint dictionary to define the constraint type spring_constraint_dict = {g1: \"<=\", g2: \"<=\", g3: \"<=\", g4: \"<=\"} # create a constrained version of the original function to be optimized spring_problem_constrained = penalty_constraints(spring_problem, spring_constraint_dict, find_minimum=True, p_count=0, p_quadratic=2) # instantiate an optimization algorithm with the constrained function alg = flying_foxes_algorithm(spring_problem_constrained) alg.b_lower = [0.05, 0.25, 2.0] alg.b_upper = [2.0, 1.3, 15.0] alg.max_iter = 200 # solve and show your results! alg.solve() print(alg.best_value) print(alg.best_position) print(alg.completed_iter) With this code, it can be found that the best solution is approximately: f(0.05269, 0.3834, 9.7789) = 0.012579","title":"Example 1:"},{"location":"penalty_constraints/#example-2","text":"In this generic problem, the constraints are a bit more difficult. The constraint_dict parameter for the penalty constraints function requires the constraints to be compared to zero. We must re-arrange the constraint equations to ensure this. Minimize: f(x, y, z) = -2x - 3y - 4z Subject to: 3x + 2y + z \u2264 10 2x + 5y + 3z \u2264 15 x, y, z \u2265 0 Re-arranged constraints: 3x + 2y + z - 10 \u2264 0 2x + 5y + 3z - 15 \u2264 0 x, y, z \u2265 0 from optiseek.modelhelpers import penalty_constraints from optiseek.metaheuristics import flying_foxes_algorithm # create the function definition def example_function(x, y, z): return -2 * x - 3 * y - 4 * z # create the constraints as functions def g1(x, y, z): return 3 * x + 2 * y + z - 10 def g2(x, y, z): return 2 * x + 5 * y + 3 * z - 15 def g3(x, y, z): return min(x, y, z) # create the constraint dictionary to define the constraint type constraint_dict = {g1: \"<=\", g2: \"<=\", g3: \">=\"} # create a constrained version of the original function to be optimized example_function_constrained = penalty_constraints(example_function, constraint_dict, p_quadratic=20) # instantiate an optimization algorithm with the constrained function alg = flying_foxes_algorithm(example_function_constrained) alg.b_lower = [0, 0, 0] alg.b_upper = [100, 100, 100] alg.max_iter = 200 # solve and show your results! alg.solve() print(alg.best_value) print(alg.best_position) print(alg.completed_iter) With this code, it can be found that the best solution is: f(0, 0, 5) = 20","title":"Example 2:"},{"location":"penalty_constraints/#references","text":"Algorithms for Optimization by Kochenderfer and Wheeler, Chapter 10.7 Spring Example Problem by Xin-She Yang","title":"References"},{"location":"rosenbrock/","text":"Rosenbrock's Function This is a 2D function with a global minimum of zero at [1, 1]. Form of the function is as follows: f(x, y) = (1 - x1)^2 + 5(x2 - x1^2)^2 function optiseek.testfunctions. rosenbrock ( x1, x2 ) Parameters Parameter Description x1 : float Input value for the first dimension. x2 : float Input value for the second dimension. Example from optiseek.testfunctions import rosenbrock y = rosenbrock(1, 1) References List of Test Functions on Wikipedia","title":"Rosenbrock Function"},{"location":"rosenbrock/#rosenbrocks-function","text":"This is a 2D function with a global minimum of zero at [1, 1]. Form of the function is as follows: f(x, y) = (1 - x1)^2 + 5(x2 - x1^2)^2 function optiseek.testfunctions. rosenbrock ( x1, x2 )","title":"Rosenbrock's Function"},{"location":"rosenbrock/#parameters","text":"Parameter Description x1 : float Input value for the first dimension. x2 : float Input value for the second dimension.","title":"Parameters"},{"location":"rosenbrock/#example","text":"from optiseek.testfunctions import rosenbrock y = rosenbrock(1, 1)","title":"Example"},{"location":"rosenbrock/#references","text":"List of Test Functions on Wikipedia","title":"References"},{"location":"simulated_annealing/","text":"Simulated Annealing This class represents the simulated annealing algorithm developed by Kirkpatrick et al. This is a local search method that takes inspiration from the annealing process in metals. Unlike deterministic gradient-based search methods, this algorithm has the ability to avoid being trapped in local optima. This is accomplished because there is a probability that a worse solution could be accepted during each iteration. As the iterations progress (i.e. temperature decreases), this probability diminishes and the algorithm is able to settle into what is hopefully a global optimum. class optiseek.metaheuristics. simulated_annealing ( input_function, b_lower=-10, b_upper=10, find_minimum=True, max_iter=100, sol_threshold=None, max_unchanged_iter=None, sigma_coeff=0.2, neighbor_dim_changes=1, initial_guess=None, store_results=False, start_temperature=10, alpha=0.9 ) Parameters Parameter Description input_function : function Function that the algorithm will use to search for an optimum. *args will be passed to the function within the solver. b_lower : float, list of floats, or ndarray Contains the lower bounds of each dimension in the search space. Can be a float if the function is one-dimensional. b_upper : float, list of floats, or ndarray Contains the upper bounds of each dimension in the search space. Can be a float if the function is one-dimensional. find_minimum : bool Indicates whether the optimimum of interest is a minimum or maximum. If true, looks for minimum. If false, looks for maximum. max_iter : int Maximum number of iterations. If reached, the algorithm terminates. sol_threshold : float If a solution is found better than this threshold, the iterations stop. None indicates that the algorithm will not consider this. max_unchanged_iter : int If the solution does not improve after this many iterations, the solver terminates. None indicates that the algorithm will not consider this. sigma_coeff : float Coefficient in (0, 0.5] to be multiplied by the bound widths for each dimension; the corresponding number is used for the standard deviation in the neighbor generation process. neighbor_dim_changes : int Number of dimensions to mutate during the generation of a new neighbor position. Must be in [1, number of dimensions] initial_guess : list of floats or ndarray Initial guess used in the solution process. Leave as None to start with a random initial guess. store_results : bool Choose whether to save intermediate iteration results for post-processing or not. If true, results will be saved. start_temperature : float Initial temperature to start iterations with. alpha : float Temperature decay coefficient in [0.6, 1). The current temperature is multiplied by this at the end of each iteration. Attributes Attribute Description best_position : ndarray Most optimal position found during the solution iterations. best_value : float Most optimal function value found during the solution iterations. completed_iter : int Number of iterations completed during the solution process. stored_positions : ndarray Positions for each member of the population for each iteration after the solver is finished. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, position in each dimension] stored_values : ndarray Function values for each member of the population for each iteration. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, function value] Methods .solve() Executes the algorithm solution with the current parameters. Results will be stored to the class attributes. If the user opted to store intermediate results, these will also be stored. Parameters None Returns None Example from optiseek.metaheuristics import simulated_annealing from optiseek.testfunctions import booth # create an instance of the algorithm, set its parameters, and solve alg = simulated_annealing(booth) # create instance with booth test function alg.b_lower = [-10, -10] # define lower bounds alg.b_upper = [10, 10] # define upper bounds alg.max_iter = 100 # set iteration limit alg.sol_threshold = 0.001 # set a solution threshold alg.sigma_coeff = 0.02 # set a multiplier of bound widths for std. dev. alg.neighbor_dim_changes = 1 # only mutate 1 dimension at a time for neighbors alg.initial_guess = [2, 5] # set an initial guess of the optimum alg.start_temperature = 5 # start the temperature at 5 alg.alpha = 0.925 # set the temperature decay rate # execute the algorithm alg.solve() # show the results! print(alg.best_value) print(alg.best_position) print(alg.completed_iter) References Simulated Annealing on Wikipedia","title":"Simulated Annealing"},{"location":"simulated_annealing/#simulated-annealing","text":"This class represents the simulated annealing algorithm developed by Kirkpatrick et al. This is a local search method that takes inspiration from the annealing process in metals. Unlike deterministic gradient-based search methods, this algorithm has the ability to avoid being trapped in local optima. This is accomplished because there is a probability that a worse solution could be accepted during each iteration. As the iterations progress (i.e. temperature decreases), this probability diminishes and the algorithm is able to settle into what is hopefully a global optimum. class optiseek.metaheuristics. simulated_annealing ( input_function, b_lower=-10, b_upper=10, find_minimum=True, max_iter=100, sol_threshold=None, max_unchanged_iter=None, sigma_coeff=0.2, neighbor_dim_changes=1, initial_guess=None, store_results=False, start_temperature=10, alpha=0.9 )","title":"Simulated Annealing"},{"location":"simulated_annealing/#parameters","text":"Parameter Description input_function : function Function that the algorithm will use to search for an optimum. *args will be passed to the function within the solver. b_lower : float, list of floats, or ndarray Contains the lower bounds of each dimension in the search space. Can be a float if the function is one-dimensional. b_upper : float, list of floats, or ndarray Contains the upper bounds of each dimension in the search space. Can be a float if the function is one-dimensional. find_minimum : bool Indicates whether the optimimum of interest is a minimum or maximum. If true, looks for minimum. If false, looks for maximum. max_iter : int Maximum number of iterations. If reached, the algorithm terminates. sol_threshold : float If a solution is found better than this threshold, the iterations stop. None indicates that the algorithm will not consider this. max_unchanged_iter : int If the solution does not improve after this many iterations, the solver terminates. None indicates that the algorithm will not consider this. sigma_coeff : float Coefficient in (0, 0.5] to be multiplied by the bound widths for each dimension; the corresponding number is used for the standard deviation in the neighbor generation process. neighbor_dim_changes : int Number of dimensions to mutate during the generation of a new neighbor position. Must be in [1, number of dimensions] initial_guess : list of floats or ndarray Initial guess used in the solution process. Leave as None to start with a random initial guess. store_results : bool Choose whether to save intermediate iteration results for post-processing or not. If true, results will be saved. start_temperature : float Initial temperature to start iterations with. alpha : float Temperature decay coefficient in [0.6, 1). The current temperature is multiplied by this at the end of each iteration.","title":"Parameters"},{"location":"simulated_annealing/#attributes","text":"Attribute Description best_position : ndarray Most optimal position found during the solution iterations. best_value : float Most optimal function value found during the solution iterations. completed_iter : int Number of iterations completed during the solution process. stored_positions : ndarray Positions for each member of the population for each iteration after the solver is finished. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, position in each dimension] stored_values : ndarray Function values for each member of the population for each iteration. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, function value]","title":"Attributes"},{"location":"simulated_annealing/#methods","text":".solve() Executes the algorithm solution with the current parameters. Results will be stored to the class attributes. If the user opted to store intermediate results, these will also be stored. Parameters None Returns None","title":"Methods"},{"location":"simulated_annealing/#example","text":"from optiseek.metaheuristics import simulated_annealing from optiseek.testfunctions import booth # create an instance of the algorithm, set its parameters, and solve alg = simulated_annealing(booth) # create instance with booth test function alg.b_lower = [-10, -10] # define lower bounds alg.b_upper = [10, 10] # define upper bounds alg.max_iter = 100 # set iteration limit alg.sol_threshold = 0.001 # set a solution threshold alg.sigma_coeff = 0.02 # set a multiplier of bound widths for std. dev. alg.neighbor_dim_changes = 1 # only mutate 1 dimension at a time for neighbors alg.initial_guess = [2, 5] # set an initial guess of the optimum alg.start_temperature = 5 # start the temperature at 5 alg.alpha = 0.925 # set the temperature decay rate # execute the algorithm alg.solve() # show the results! print(alg.best_value) print(alg.best_position) print(alg.completed_iter)","title":"Example"},{"location":"simulated_annealing/#references","text":"Simulated Annealing on Wikipedia","title":"References"},{"location":"tabu_search/","text":"Tabu Search This class represents the tabu search algorithm developed by Glover. This is a local search algorithm that utilizes memory of past positions in order to avoid getting stuck in local optima. The algorithm finds a new neighbor at each iteration; if the neighbor matches a previous position on the \"tabu list\" (within a specified threshold), then that position is prohibited unless it is better than the currently best known position. The tabu list consists of positions from a specified number of previous iterations. The length of the tabu list and tabu threshold can both be tuned. class optiseek.metaheuristics. tabu_search ( input_function, b_lower=-10, b_upper=10, find_minimum=True, max_iter=100, sol_threshold=None, max_unchanged_iter=None, sigma_coeff=0.2, neighbor_dim_changes=1, initial_guess=None, store_results=False, tenure=5, n_candidates=5, neighbor_tolerance=0.02 ) Parameters Parameter Description input_function : function Function that the algorithm will use to search for an optimum. *args will be passed to the function within the solver. b_lower : float, list of floats, or ndarray Contains the lower bounds of each dimension in the search space. Can be a float if the function is one-dimensional. b_upper : float, list of floats, or ndarray Contains the upper bounds of each dimension in the search space. Can be a float if the function is one-dimensional. find_minimum : bool Indicates whether the optimimum of interest is a minimum or maximum. If true, looks for minimum. If false, looks for maximum. max_iter : int Maximum number of iterations. If reached, the algorithm terminates. sol_threshold : float If a solution is found better than this threshold, the iterations stop. None indicates that the algorithm will not consider this. max_unchanged_iter : int If the solution does not improve after this many iterations, the solver terminates. None indicates that the algorithm will not consider this. sigma_coeff : float Coefficient in (0, 0.5] to be multiplied by the bound widths for each dimension; the corresponding number is used for the standard deviation in the neighbor generation process. neighbor_dim_changes : int Number of dimensions to mutate during the generation of a new neighbor position. Must be in [1, number of dimensions] initial_guess : list of floats or ndarray Initial guess used in the solution process. Leave as None to start with a random initial guess. store_results : bool Choose whether to save intermediate iteration results for post-processing or not. If true, results will be saved. tenure : int Number of previous positions stored on the tabu list. These positions (within a specified tolerance) will be prohibited in following iterations. n_candidates : int Number of new candidate solutions to guess at each iteration. The best solution that is not tabu is used. neighbor_tolerance : float Portion of dimension width to use as a tolerance when determining whether a potential position is tabu. Attributes Attribute Description best_position : ndarray Most optimal position found during the solution iterations. best_value : float Most optimal function value found during the solution iterations. completed_iter : int Number of iterations completed during the solution process. stored_positions : ndarray Positions for each member of the population for each iteration after the solver is finished. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, position in each dimension] stored_values : ndarray Function values for each member of the population for each iteration. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, function value] Methods .solve() Executes the algorithm solution with the current parameters. Results will be stored to the class attributes. If the user opted to store intermediate results, these will also be stored. Parameters None Returns None Example from optiseek.metaheuristics import tabu_search from optiseek.testfunctions import booth # create an instance of the algorithm, set its parameters, and solve alg = tabu_search(booth) # create instance with booth test function alg.b_lower = [-10, -10] # define lower bounds alg.b_upper = [10, 10] # define upper bounds alg.max_iter = 100 # set iteration limit alg.sol_threshold = 0.001 # set a solution threshold alg.sigma_coeff = 0.02 # set a multiplier of bound widths for std. dev. alg.neighbor_dim_changes = 1 # only mutate 1 dimension at a time for neighbors alg.initial_guess = [2, 5] # set an initial guess of the optimum alg.tenure = 7 # set the length of the tabu list to 7 previous positions alg.n_candidates = 4 # generate four neighbors during each iteration and pick best alg.neighbor_tolerance = 0.015 # fraction of bound widths to use for tolerance # execute the algorithm alg.solve() # show the results! print(alg.best_value) print(alg.best_position) print(alg.completed_iter) References Tabu Search on Wikipedia","title":"Tabu Search"},{"location":"tabu_search/#tabu-search","text":"This class represents the tabu search algorithm developed by Glover. This is a local search algorithm that utilizes memory of past positions in order to avoid getting stuck in local optima. The algorithm finds a new neighbor at each iteration; if the neighbor matches a previous position on the \"tabu list\" (within a specified threshold), then that position is prohibited unless it is better than the currently best known position. The tabu list consists of positions from a specified number of previous iterations. The length of the tabu list and tabu threshold can both be tuned. class optiseek.metaheuristics. tabu_search ( input_function, b_lower=-10, b_upper=10, find_minimum=True, max_iter=100, sol_threshold=None, max_unchanged_iter=None, sigma_coeff=0.2, neighbor_dim_changes=1, initial_guess=None, store_results=False, tenure=5, n_candidates=5, neighbor_tolerance=0.02 )","title":"Tabu Search"},{"location":"tabu_search/#parameters","text":"Parameter Description input_function : function Function that the algorithm will use to search for an optimum. *args will be passed to the function within the solver. b_lower : float, list of floats, or ndarray Contains the lower bounds of each dimension in the search space. Can be a float if the function is one-dimensional. b_upper : float, list of floats, or ndarray Contains the upper bounds of each dimension in the search space. Can be a float if the function is one-dimensional. find_minimum : bool Indicates whether the optimimum of interest is a minimum or maximum. If true, looks for minimum. If false, looks for maximum. max_iter : int Maximum number of iterations. If reached, the algorithm terminates. sol_threshold : float If a solution is found better than this threshold, the iterations stop. None indicates that the algorithm will not consider this. max_unchanged_iter : int If the solution does not improve after this many iterations, the solver terminates. None indicates that the algorithm will not consider this. sigma_coeff : float Coefficient in (0, 0.5] to be multiplied by the bound widths for each dimension; the corresponding number is used for the standard deviation in the neighbor generation process. neighbor_dim_changes : int Number of dimensions to mutate during the generation of a new neighbor position. Must be in [1, number of dimensions] initial_guess : list of floats or ndarray Initial guess used in the solution process. Leave as None to start with a random initial guess. store_results : bool Choose whether to save intermediate iteration results for post-processing or not. If true, results will be saved. tenure : int Number of previous positions stored on the tabu list. These positions (within a specified tolerance) will be prohibited in following iterations. n_candidates : int Number of new candidate solutions to guess at each iteration. The best solution that is not tabu is used. neighbor_tolerance : float Portion of dimension width to use as a tolerance when determining whether a potential position is tabu.","title":"Parameters"},{"location":"tabu_search/#attributes","text":"Attribute Description best_position : ndarray Most optimal position found during the solution iterations. best_value : float Most optimal function value found during the solution iterations. completed_iter : int Number of iterations completed during the solution process. stored_positions : ndarray Positions for each member of the population for each iteration after the solver is finished. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, position in each dimension] stored_values : ndarray Function values for each member of the population for each iteration. Set to None if user does not choose to store results. The results are placed in an array in the following format: [iteration, population member, function value]","title":"Attributes"},{"location":"tabu_search/#methods","text":".solve() Executes the algorithm solution with the current parameters. Results will be stored to the class attributes. If the user opted to store intermediate results, these will also be stored. Parameters None Returns None","title":"Methods"},{"location":"tabu_search/#example","text":"from optiseek.metaheuristics import tabu_search from optiseek.testfunctions import booth # create an instance of the algorithm, set its parameters, and solve alg = tabu_search(booth) # create instance with booth test function alg.b_lower = [-10, -10] # define lower bounds alg.b_upper = [10, 10] # define upper bounds alg.max_iter = 100 # set iteration limit alg.sol_threshold = 0.001 # set a solution threshold alg.sigma_coeff = 0.02 # set a multiplier of bound widths for std. dev. alg.neighbor_dim_changes = 1 # only mutate 1 dimension at a time for neighbors alg.initial_guess = [2, 5] # set an initial guess of the optimum alg.tenure = 7 # set the length of the tabu list to 7 previous positions alg.n_candidates = 4 # generate four neighbors during each iteration and pick best alg.neighbor_tolerance = 0.015 # fraction of bound widths to use for tolerance # execute the algorithm alg.solve() # show the results! print(alg.best_value) print(alg.best_position) print(alg.completed_iter)","title":"Example"},{"location":"tabu_search/#references","text":"Tabu Search on Wikipedia","title":"References"},{"location":"wheelers_ridge/","text":"Wheeler's Ridge This is a 2D function with a global minimum in a deep valley. It is mostly smooth other than two ridges along each of the principal axes. These cause some algorithms to converge into local minima or diverge. In this form, the minimum is at [1, 1.5] with a value of -1. Form of the function is as follows: f(x, y) = -exp(-(x1*x2 - 1.5)^2 - (x2 - 1.5)^2) function optiseek.testfunctions. wheelers_ridge ( x1, x2 ) Parameters Parameter Description x1 : float Input value for the first dimension. x2 : float Input value for the second dimension. Example from optiseek.testfunctions import wheelers_ridge y = wheelers_ridge(1, 1.5) References List of Test Functions on Wikipedia","title":"Wheelers Ridge"},{"location":"wheelers_ridge/#wheelers-ridge","text":"This is a 2D function with a global minimum in a deep valley. It is mostly smooth other than two ridges along each of the principal axes. These cause some algorithms to converge into local minima or diverge. In this form, the minimum is at [1, 1.5] with a value of -1. Form of the function is as follows: f(x, y) = -exp(-(x1*x2 - 1.5)^2 - (x2 - 1.5)^2) function optiseek.testfunctions. wheelers_ridge ( x1, x2 )","title":"Wheeler's  Ridge"},{"location":"wheelers_ridge/#parameters","text":"Parameter Description x1 : float Input value for the first dimension. x2 : float Input value for the second dimension.","title":"Parameters"},{"location":"wheelers_ridge/#example","text":"from optiseek.testfunctions import wheelers_ridge y = wheelers_ridge(1, 1.5)","title":"Example"},{"location":"wheelers_ridge/#references","text":"List of Test Functions on Wikipedia","title":"References"}]}